{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "average_sequential.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQkH9OF8reo7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "X_train = X_train.reshape(len(X_train), np.prod(X_train.shape[1:]))\n",
        "X_test = X_test.reshape(len(X_test), np.prod(X_test.shape[1:]))\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "inChannel = 1\n",
        "x, y = 28, 28\n",
        "input_img = Input(shape = (x, y, inChannel))\n",
        "num_classes = 10\n",
        "\n",
        "X_train = X_train/np.max(X_train)\n",
        "X_test = X_test/np.max(X_test)\n",
        "\n",
        "print(np.max(X_train))\n",
        "train_X,valid_X,train_ground,valid_ground = train_test_split(X_train,\n",
        "                                                             X_train,\n",
        "                                                             test_size=0.2,\n",
        "                                                             random_state=13)\n",
        "\n",
        "input_img = Input(shape = (784,))\n",
        "num_classes = 10\n",
        "\n",
        "def Encoder_part(input_img):\n",
        "\n",
        "    encoded = Dense(units=256, activation='relu')(input_img)\n",
        "    encoded = Dense(units=128, activation='relu')(encoded)\n",
        "    encoded = Dense(units=64, activation='relu')(encoded)\n",
        "    encoded = Dense(units=32, activation='relu')(encoded)\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def Decoder_part(encoded):\n",
        "        \n",
        "    decoded = Dense(units=64, activation='relu')(encoded)\n",
        "    decoded = Dense(units=128, activation='relu')(decoded)\n",
        "    decoded = Dense(units=256, activation='relu')(decoded)\n",
        "    decoded = Dense(units=784, activation='relu')(decoded)\n",
        "    \n",
        "    return decoded"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URijeWSJ1uON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder1=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "autoencoder2=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder3=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder4=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder5=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "autoencoder_final=Model(input_img, Decoder_part(Encoder_part(input_img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAloY0wzso1i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#encoder = Model(input_img, Encoder_part(input_img))\n",
        "\n",
        "autoencoder1.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "autoencoder2.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder3.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder4.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder5.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "autoencoder_final.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "\n",
        "autoencoder1.summary()\n",
        "autoencoder2.summary()\n",
        "#autoencoder3.summary()\n",
        "#autoencoder4.summary()\n",
        "#autoencoder5.summary()\n",
        "autoencoder_final.summary()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwpv6AsLR7pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(train_X[1][1])\n",
        "#print(np.shape(train_X))\n",
        "batch_train_X = []\n",
        "batch_train_ground = []\n",
        "\n",
        "i=0\n",
        "while i < 48000:\n",
        "  x  = []\n",
        "  j = 0\n",
        "  while j<1000:\n",
        "    x.append(train_X[i])\n",
        "    j = j+1\n",
        "    i = i+1\n",
        "  print(i)\n",
        "  batch_train_X.append(x)\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAyIO94uYz4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np.shape(train_X))\n",
        "print(np.shape(batch_train_ground))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdl94PI_9ig8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "i=0\n",
        "while i < 48000:\n",
        "  x  = []\n",
        "  j = 0\n",
        "  while j<1000:\n",
        "    x.append(train_ground[i])\n",
        "    j = j+1\n",
        "    i = i+1\n",
        "  print(i)\n",
        "  batch_train_ground.append(x)\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L066HK2Hd6hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(batch_train_X[47][999])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3C8-BkEXo-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_X = valid_X[:1000]\n",
        "valid_ground = valid_ground[:1000]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuqOcgB7i8r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.shape(train_X))\n",
        "print(np.shape(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpqUI8478ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "27c7dfae-9b22-4bcf-a120-7fe69f64f643"
      },
      "source": [
        "count = 0\n",
        "number = 0\n",
        "for epo in range (0,50):\n",
        "  number = 0\n",
        "  print(\"entered first loop : \", epo)\n",
        "  for i,j in zip(batch_train_X,batch_train_ground):\n",
        "    print(\"entered second loop : \")\n",
        "    number = number + 1\n",
        "    i = [i]\n",
        "    j = [j]\n",
        "    print(\"**************************************************************************************************************8\")\n",
        "    print(\"batch : \" , number, \"epoch : \", epo)\n",
        "    if count == 0:\n",
        "      print(\"Entered if 1\", count)\n",
        "      autoencoder_train1 = autoencoder1.fit(i,j, batch_size=1000,epochs=1,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "      count = count+1\n",
        "    if count == 1:\n",
        "      print(\"Entered if 2\", count)\n",
        "      autoencoder_train2 = autoencoder2.fit(i,j, batch_size=1000,epochs=1,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "      count = count+1\n",
        "    #if count == 2:  \n",
        "    #  autoencoder_train3 = autoencoder3.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    #if count == 3:  \n",
        "    #  autoencoder_train4 = autoencoder4.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    #if count ==4:  \n",
        "    #  autoencoder_train5 = autoencoder5.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    if count ==2:\n",
        "      print(\"Entered if 3\", count)\n",
        "      count = 0\n",
        "      w1 = np.array(autoencoder1.get_weights())\n",
        "      w2 = np.array(autoencoder2.get_weights())\n",
        "      #w3 = autoencoder3.get_weights()\n",
        "      #w4 = autoencoder4.get_weights()\n",
        "      #w5 = autoencoder5.get_weights()\n",
        "      w_final = (w1+w2)/2\n",
        "      \n",
        "      print(\"Taken weights\")\n",
        "      \n",
        "      w_cur = np.array(autoencoder_final.get_weights())\n",
        "      w_final = (w_cur+w_final)/2\n",
        "\n",
        "      \n",
        "    \n",
        "      print(\"Meaned weights\")\n",
        "    \n",
        "      \n",
        "    \n",
        "      autoencoder_final.set_weights(w_final)\n",
        "      autoencoder1.set_weights(w_final)\n",
        "      autoencoder2.set_weights(w_final)\n",
        "      #autoencoder3.set_weights(w_final)\n",
        "      #autoencoder4.set_weights(w_final)\n",
        "      #autoencoder5.set_weights(w_final)\n",
        "      #autoencoder.set_weights(w_final)\n",
        "      \n",
        "      print(\"Going to the top again\")\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  15 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1110 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1110 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  16 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1107 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1107 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  17 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1086 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1086 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  18 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1096 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1096 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  19 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1096 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1096 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  20 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  21 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1081 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1081 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  22 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  23 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1082 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1082 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  24 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1092 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1092 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  25 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1085 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1085 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  26 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  27 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1091 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1091 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  28 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  29 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1084 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 22us/step - loss: 0.1084 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  30 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1094 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1094 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  31 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1085 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1085 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  32 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1109 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  33 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1103 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1103 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  34 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 20us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1116 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  35 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1111 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 15us/step - loss: 0.1111 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  36 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 15us/step - loss: 0.1111 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1111 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  37 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1114 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1114 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  38 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1076 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1076 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  39 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1068 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1068 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  40 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1105 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1105 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  41 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1073 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1073 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  42 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1078 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1078 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  43 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1097 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1097 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  44 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1102 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1102 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  45 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 19us/step - loss: 0.1084 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1084 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  46 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1071 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 16us/step - loss: 0.1071 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  47 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1078 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1078 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  48 epoch :  49\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1087 - val_loss: 0.1114\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1087 - val_loss: 0.1114\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxxKWzsiw42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = autoencoder_train1.history['loss']\n",
        "val_loss = autoencoder1.history['val_loss']\n",
        "epochs = range(75)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss - AE1')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "loss = autoencoder_train2.history['loss']\n",
        "val_loss = autoencoder_train2.history['val_loss']\n",
        "epochs = range(100)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "weight'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2djvWZpsYHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7f4c8835-e589-4b6e-921d-9f147c73ae46"
      },
      "source": [
        "wt_fn = autoencoder_final.get_weights()\n",
        "print(\"Weights taken\")\n",
        "print(np.shape(wt_fn[0]))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights taken\n",
            "(784, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u77euOJ2uSst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "db70aa0a-3597-47b0-dc88-e7ac5b4b7e1b"
      },
      "source": [
        "train_Y_one_hot = to_categorical(y_train)\n",
        "test_Y_one_hot = to_categorical(y_test)\n",
        "print('Original label:', y_train[0])\n",
        "print('After conversion to one-hot:', train_Y_one_hot[0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original label: 5\n",
            "After conversion to one-hot: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMaMoQAxAsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X,valid_X,y_train,y_valid = train_test_split(X_train,train_Y_one_hot,test_size=0.2,random_state=13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SBrsWJpxDLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(enco):\n",
        "    flat = enco\n",
        "    den = Dense(64, activation='relu')(flat)\n",
        "    out = Dense(num_classes, activation='softmax')(den)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-q3pY9mxIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model = Model(input_img,fc(Encoder_part(input_img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvQoDW9xMAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l1,l2 in zip(full_model.layers[:6],autoencoder_final.layers[0:6]):\n",
        "    l1.set_weights(l2.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOXgqx1qx3Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:6]:\n",
        "    layer.trainable = False\n",
        "full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf92vduBx_Lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "523b4ec9-3c1d-4f92-abb4-1ce9117349a3"
      },
      "source": [
        "full_model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_32 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 246,954\n",
            "Trainable params: 650\n",
            "Non-trainable params: 246,304\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77RiAnPdx_qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:6]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P_Ki51gyF5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3abab848-4d31-4500-8b36-cd072146d64c"
      },
      "source": [
        "classify_train = full_model.fit(train_X, y_train, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, y_valid))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0710 07:28:24.440741 139751531292544 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.2883 - acc: 0.1894 - val_loss: 2.2745 - val_acc: 0.2433\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.2616 - acc: 0.2506 - val_loss: 2.2492 - val_acc: 0.2672\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.2368 - acc: 0.2655 - val_loss: 2.2250 - val_acc: 0.2845\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.2133 - acc: 0.2892 - val_loss: 2.2023 - val_acc: 0.2913\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.1909 - acc: 0.2970 - val_loss: 2.1805 - val_acc: 0.3098\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.1697 - acc: 0.3066 - val_loss: 2.1597 - val_acc: 0.3304\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.1497 - acc: 0.3279 - val_loss: 2.1401 - val_acc: 0.3335\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.1306 - acc: 0.3339 - val_loss: 2.1217 - val_acc: 0.3434\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 2.1123 - acc: 0.3468 - val_loss: 2.1038 - val_acc: 0.3450\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 2.0948 - acc: 0.3533 - val_loss: 2.0867 - val_acc: 0.3513\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0782 - acc: 0.3574 - val_loss: 2.0705 - val_acc: 0.3599\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0622 - acc: 0.3636 - val_loss: 2.0552 - val_acc: 0.3663\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0470 - acc: 0.3697 - val_loss: 2.0405 - val_acc: 0.3698\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0324 - acc: 0.3755 - val_loss: 2.0261 - val_acc: 0.3724\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0185 - acc: 0.3776 - val_loss: 2.0126 - val_acc: 0.3782\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 2.0051 - acc: 0.3838 - val_loss: 1.9995 - val_acc: 0.3819\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.9923 - acc: 0.3870 - val_loss: 1.9871 - val_acc: 0.3828\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9800 - acc: 0.3898 - val_loss: 1.9751 - val_acc: 0.3854\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9682 - acc: 0.3930 - val_loss: 1.9636 - val_acc: 0.3874\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9568 - acc: 0.3949 - val_loss: 1.9527 - val_acc: 0.3923\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9459 - acc: 0.3985 - val_loss: 1.9419 - val_acc: 0.3947\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9354 - acc: 0.3997 - val_loss: 1.9318 - val_acc: 0.3987\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9253 - acc: 0.4044 - val_loss: 1.9221 - val_acc: 0.3995\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9156 - acc: 0.4040 - val_loss: 1.9125 - val_acc: 0.4039\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.9063 - acc: 0.4082 - val_loss: 1.9035 - val_acc: 0.4045\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.8973 - acc: 0.4093 - val_loss: 1.8948 - val_acc: 0.4054\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8887 - acc: 0.4107 - val_loss: 1.8865 - val_acc: 0.4090\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8803 - acc: 0.4134 - val_loss: 1.8784 - val_acc: 0.4098\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8722 - acc: 0.4152 - val_loss: 1.8705 - val_acc: 0.4112\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8645 - acc: 0.4151 - val_loss: 1.8630 - val_acc: 0.4127\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8569 - acc: 0.4172 - val_loss: 1.8559 - val_acc: 0.4149\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8497 - acc: 0.4190 - val_loss: 1.8487 - val_acc: 0.4160\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.8426 - acc: 0.4191 - val_loss: 1.8419 - val_acc: 0.4177\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8358 - acc: 0.4208 - val_loss: 1.8352 - val_acc: 0.4207\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8291 - acc: 0.4228 - val_loss: 1.8287 - val_acc: 0.4196\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8227 - acc: 0.4237 - val_loss: 1.8223 - val_acc: 0.4198\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8164 - acc: 0.4245 - val_loss: 1.8163 - val_acc: 0.4222\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8104 - acc: 0.4255 - val_loss: 1.8104 - val_acc: 0.4223\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.8045 - acc: 0.4272 - val_loss: 1.8047 - val_acc: 0.4233\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7989 - acc: 0.4271 - val_loss: 1.7993 - val_acc: 0.4254\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7933 - acc: 0.4288 - val_loss: 1.7939 - val_acc: 0.4267\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7880 - acc: 0.4294 - val_loss: 1.7887 - val_acc: 0.4260\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7828 - acc: 0.4312 - val_loss: 1.7836 - val_acc: 0.4265\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7777 - acc: 0.4316 - val_loss: 1.7786 - val_acc: 0.4274\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7728 - acc: 0.4325 - val_loss: 1.7739 - val_acc: 0.4279\n",
            "Epoch 46/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7680 - acc: 0.4326 - val_loss: 1.7692 - val_acc: 0.4297\n",
            "Epoch 47/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7633 - acc: 0.4332 - val_loss: 1.7647 - val_acc: 0.4298\n",
            "Epoch 48/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7587 - acc: 0.4352 - val_loss: 1.7604 - val_acc: 0.4308\n",
            "Epoch 49/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7543 - acc: 0.4366 - val_loss: 1.7559 - val_acc: 0.4313\n",
            "Epoch 50/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7500 - acc: 0.4362 - val_loss: 1.7517 - val_acc: 0.4325\n",
            "Epoch 51/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7458 - acc: 0.4371 - val_loss: 1.7476 - val_acc: 0.4325\n",
            "Epoch 52/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7416 - acc: 0.4386 - val_loss: 1.7435 - val_acc: 0.4348\n",
            "Epoch 53/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7376 - acc: 0.4391 - val_loss: 1.7396 - val_acc: 0.4342\n",
            "Epoch 54/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7337 - acc: 0.4408 - val_loss: 1.7357 - val_acc: 0.4346\n",
            "Epoch 55/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7299 - acc: 0.4396 - val_loss: 1.7322 - val_acc: 0.4365\n",
            "Epoch 56/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7261 - acc: 0.4414 - val_loss: 1.7285 - val_acc: 0.4380\n",
            "Epoch 57/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7224 - acc: 0.4428 - val_loss: 1.7249 - val_acc: 0.4363\n",
            "Epoch 58/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.7189 - acc: 0.4431 - val_loss: 1.7214 - val_acc: 0.4377\n",
            "Epoch 59/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.7154 - acc: 0.4432 - val_loss: 1.7181 - val_acc: 0.4391\n",
            "Epoch 60/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.7120 - acc: 0.4447 - val_loss: 1.7147 - val_acc: 0.4399\n",
            "Epoch 61/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7087 - acc: 0.4462 - val_loss: 1.7114 - val_acc: 0.4391\n",
            "Epoch 62/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7054 - acc: 0.4456 - val_loss: 1.7082 - val_acc: 0.4397\n",
            "Epoch 63/100\n",
            "48000/48000 [==============================] - 3s 72us/step - loss: 1.7022 - acc: 0.4465 - val_loss: 1.7052 - val_acc: 0.4407\n",
            "Epoch 64/100\n",
            "48000/48000 [==============================] - 3s 69us/step - loss: 1.6990 - acc: 0.4468 - val_loss: 1.7020 - val_acc: 0.4417\n",
            "Epoch 65/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6959 - acc: 0.4477 - val_loss: 1.6991 - val_acc: 0.4425\n",
            "Epoch 66/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6929 - acc: 0.4481 - val_loss: 1.6961 - val_acc: 0.4428\n",
            "Epoch 67/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6900 - acc: 0.4489 - val_loss: 1.6932 - val_acc: 0.4438\n",
            "Epoch 68/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6870 - acc: 0.4499 - val_loss: 1.6904 - val_acc: 0.4447\n",
            "Epoch 69/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6842 - acc: 0.4499 - val_loss: 1.6876 - val_acc: 0.4454\n",
            "Epoch 70/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6814 - acc: 0.4506 - val_loss: 1.6848 - val_acc: 0.4442\n",
            "Epoch 71/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6786 - acc: 0.4514 - val_loss: 1.6822 - val_acc: 0.4450\n",
            "Epoch 72/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6759 - acc: 0.4519 - val_loss: 1.6795 - val_acc: 0.4467\n",
            "Epoch 73/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6733 - acc: 0.4525 - val_loss: 1.6770 - val_acc: 0.4477\n",
            "Epoch 74/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6707 - acc: 0.4528 - val_loss: 1.6744 - val_acc: 0.4484\n",
            "Epoch 75/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6682 - acc: 0.4537 - val_loss: 1.6720 - val_acc: 0.4484\n",
            "Epoch 76/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6657 - acc: 0.4533 - val_loss: 1.6695 - val_acc: 0.4492\n",
            "Epoch 77/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6632 - acc: 0.4550 - val_loss: 1.6670 - val_acc: 0.4494\n",
            "Epoch 78/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6607 - acc: 0.4549 - val_loss: 1.6648 - val_acc: 0.4498\n",
            "Epoch 79/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6584 - acc: 0.4556 - val_loss: 1.6622 - val_acc: 0.4501\n",
            "Epoch 80/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6560 - acc: 0.4556 - val_loss: 1.6600 - val_acc: 0.4503\n",
            "Epoch 81/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6537 - acc: 0.4565 - val_loss: 1.6577 - val_acc: 0.4510\n",
            "Epoch 82/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6514 - acc: 0.4572 - val_loss: 1.6556 - val_acc: 0.4515\n",
            "Epoch 83/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6492 - acc: 0.4582 - val_loss: 1.6533 - val_acc: 0.4521\n",
            "Epoch 84/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6470 - acc: 0.4583 - val_loss: 1.6511 - val_acc: 0.4531\n",
            "Epoch 85/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6448 - acc: 0.4591 - val_loss: 1.6490 - val_acc: 0.4533\n",
            "Epoch 86/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6427 - acc: 0.4590 - val_loss: 1.6468 - val_acc: 0.4537\n",
            "Epoch 87/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6406 - acc: 0.4596 - val_loss: 1.6448 - val_acc: 0.4543\n",
            "Epoch 88/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6385 - acc: 0.4602 - val_loss: 1.6428 - val_acc: 0.4547\n",
            "Epoch 89/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6365 - acc: 0.4606 - val_loss: 1.6408 - val_acc: 0.4557\n",
            "Epoch 90/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6344 - acc: 0.4611 - val_loss: 1.6389 - val_acc: 0.4557\n",
            "Epoch 91/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6325 - acc: 0.4619 - val_loss: 1.6370 - val_acc: 0.4561\n",
            "Epoch 92/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6305 - acc: 0.4626 - val_loss: 1.6350 - val_acc: 0.4563\n",
            "Epoch 93/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6286 - acc: 0.4629 - val_loss: 1.6331 - val_acc: 0.4573\n",
            "Epoch 94/100\n",
            "48000/48000 [==============================] - 3s 67us/step - loss: 1.6267 - acc: 0.4631 - val_loss: 1.6313 - val_acc: 0.4574\n",
            "Epoch 95/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6248 - acc: 0.4635 - val_loss: 1.6295 - val_acc: 0.4573\n",
            "Epoch 96/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6229 - acc: 0.4642 - val_loss: 1.6276 - val_acc: 0.4585\n",
            "Epoch 97/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6211 - acc: 0.4644 - val_loss: 1.6258 - val_acc: 0.4597\n",
            "Epoch 98/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6193 - acc: 0.4652 - val_loss: 1.6241 - val_acc: 0.4595\n",
            "Epoch 99/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6176 - acc: 0.4650 - val_loss: 1.6223 - val_acc: 0.4599\n",
            "Epoch 100/100\n",
            "48000/48000 [==============================] - 3s 68us/step - loss: 1.6158 - acc: 0.4659 - val_loss: 1.6206 - val_acc: 0.4602\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOqbITPayIUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model.save_weights('classification_complete.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ovj_adgzeyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "a20101d9-4b0a-40cd-d719-e721ff04261a"
      },
      "source": [
        "accuracy = classify_train.history['acc']\n",
        "val_accuracy = classify_train.history['val_acc']\n",
        "loss = classify_train.history['loss']\n",
        "val_loss = classify_train.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmcFNW5//HPw4DAsG9u7CoKwzIw\nDKABVBQMRgOJSxQh7uFqRBNjkmuiN3o1+otxiSaXnxGNxAVFYqLBJOpPUSTqVRmQRVEElX2RzQFB\n0cHn98epHptxeqZnpmd6pvv7fr361V1Vp6pPdc08ffqpU6fM3RERkezQKN0VEBGRuqOgLyKSRRT0\nRUSyiIK+iEgWUdAXEckiCvoiIllEQT8LmVmOmX1iZt1SWTadzOwIM0t5/2MzG21mq+Kml5vZyGTK\nVuO97jOzX1Z3fZFkNE53BaRyZvZJ3GQusBfYF03/h7vPqMr23H0f0DLVZbOBux+Viu2Y2cXAJHc/\nPm7bF6di2yIVUdBvANy9NOhGLcmL3f35ROXNrLG7l9RF3UQqo7/H+kXpnQxgZr82s8fM7FEz2wVM\nMrNjzOw1M/vYzDaa2e/NrElUvrGZuZn1iKYfjpY/bWa7zOx/zaxnVctGy082s/fMrNjM/mBmr5jZ\n+QnqnUwd/8PMVprZDjP7fdy6OWb2OzPbZmYfAGMr+HyuMbOZZeZNNbM7otcXm9k70f68H7XCE21r\nnZkdH73ONbOHorq9DQwuU/ZaM/sg2u7bZjYumt8f+B9gZJQ62xr32V4ft/4l0b5vM7MnzeyQZD6b\nqnzOsfqY2fNmtt3MNpnZz+Pe57+iz2SnmRWZ2aHlpdLM7OXYcY4+z3nR+2wHrjWzXmb2YvQeW6PP\nrU3c+t2jfdwSLb/LzJpFde4TV+4QM9tjZh0S7a9Uwt31aEAPYBUwusy8XwOfA98mfJE3B4YAwwi/\n5g4D3gOmROUbAw70iKYfBrYChUAT4DHg4WqUPRDYBYyPlv0E+AI4P8G+JFPHvwNtgB7A9ti+A1OA\nt4EuQAdgXvhzLvd9DgM+AVrEbfsjoDCa/nZUxoATgE+BAdGy0cCquG2tA46PXt8GzAXaAd2BZWXK\nfg84JDom50R1OChadjEwt0w9Hwauj16fFNVxINAM+L/AC8l8NlX8nNsAm4EfAU2B1sDQaNkvgMVA\nr2gfBgLtgSPKftbAy7HjHO1bCXApkEP4ezwSOBE4IPo7eQW4LW5/3oo+zxZR+eHRsmnATXHvcxXw\nRLr/DxvyI+0V0KOKByxx0H+hkvV+Cvwlel1eIP9jXNlxwFvVKHsh8O+4ZQZsJEHQT7KOR8ct/xvw\n0+j1PEKaK7bsW2UDUZltvwacE70+GVheQdl/AJdFrysK+mvijwXww/iy5Wz3LeCU6HVlQf8B4Oa4\nZa0J53G6VPbZVPFz/j4wP0G592P1LTM/maD/QSV1OCP2vsBIYBOQU0654cCHgEXTi4DTUv1/lU0P\npXcyx9r4CTPrbWb/jH6u7wRuADpWsP6muNd7qPjkbaKyh8bXw8N/6bpEG0myjkm9F7C6gvoCPAJM\niF6fE03H6nGqmb0epR4+JrSyK/qsYg6pqA5mdr6ZLY5SFB8DvZPcLoT9K92eu+8EdgCd48okdcwq\n+Zy7EoJ7eSpaVpmyf48Hm9ksM1sf1eHPZeqwykOngf24+yuEXw0jzKwf0A34ZzXrJCinn0nKdle8\nh9CyPMLdWwO/IrS8a9NGQksUADMz9g9SZdWkjhsJwSKmsi6ls4DRZtaZkH56JKpjc+Bx4P8QUi9t\ngf+XZD02JaqDmR0G3E1IcXSItvtu3HYr6166gZAyim2vFSGNtD6JepVV0ee8Fjg8wXqJlu2O6pQb\nN+/gMmXK7t8thF5n/aM6nF+mDt3NLCdBPR4EJhF+lcxy970JykkSFPQzVyugGNgdnQj7jzp4z38A\nBWb2bTNrTMgTd6qlOs4CfmxmnaOTev9ZUWF330RIQfyZkNpZES1qSsgzbwH2mdmphNxzsnX4pZm1\ntXAdw5S4ZS0JgW8L4fvvB4SWfsxmoEv8CdUyHgUuMrMBZtaU8KX0b3dP+MupAhV9zrOBbmY2xcya\nmllrMxsaLbsP+LWZHW7BQDNrT/iy20ToMJBjZpOJ+4KqoA67gWIz60pIMcX8L7ANuNnCyfHmZjY8\nbvlDhHTQOYQvAKkBBf3MdRVwHuHE6j2EE661yt03A2cBdxD+iQ8H3iS08FJdx7uBOcBSYD6htV6Z\nRwg5+tLUjrt/DFwJPEE4GXoG4csrGdcRfnGsAp4mLiC5+xLgD8AbUZmjgNfj1n0OWAFsNrP4NE1s\n/WcIaZgnovW7AROTrFdZCT9ndy8GxgCnE76I3gOOixbfCjxJ+Jx3Ek6qNovSdj8Afkk4qX9EmX0r\nz3XAUMKXz2zgr3F1KAFOBfoQWv1rCMchtnwV4TjvdfdXq7jvUkbs5IhIykU/1zcAZ7j7v9NdH2m4\nzOxBwsnh69Ndl4ZOF2dJSpnZWEJPmU8JXf6+ILR2RaolOj8yHuif7rpkAqV3JNVGAB8QctnfBL6r\nE29SXWb2fwjXCtzs7mvSXZ9MoPSOiEgWUUtfRCSL1LucfseOHb1Hjx7proaISIOyYMGCre5eURdp\noB4G/R49elBUVJTuaoiINChmVtlV6YDSOyIiWUVBX0Qkiyjoi4hkEQV9EZEsoqAvIpJFFPRFRNJs\nxgzo0QMaNQrPM2bU3nvVuy6bIiIN3YwZcM01sGYNtG8f5m3fXv7rbdvADGKDI6xeDZMnh9cTqzuu\nagXU0hcRqYL4VnnHjuER/9oMvv/9ELzdQ1Dfti3xa/gq4Mfs2RO+NGqDWvoiImVU1FLftQs+/zzM\niwXtsq9TMaTZmloaXk5BX0SySmWpl7LplkSBvbZ1q+wGoNWkoC8iGSNVAT3dgw/n5sJNN9XOthX0\nRaRByJSAXpnu3UPAr42TuKCgLyL1WCzQr17dcAN6kybwxRfJlR08GGp7vEkFfRGpE8l0Y+zWDb71\nLfjXv74e6OtLQG/cGJo2hd27QxoGQm+bFi3C6927Q2+eL78M00ceCWPGwIEHhnK7d0PLlnDwwXDI\nIWF+p06h50/btnVQ/9p/CxHJJuUF92RTL6tXw913fzVdHwJ9s2ahHnv3hmBeUhIeEIJ4zO7d4fnA\nA0OQjz0OPbTu61wRBX0RSVpN8ur1IYCXFd8iT6RJE2jTJrTEhwyBESNg6NDQKm/SJLT89+0LKZx9\n+0ILvlE9vgIqqaBvZmOBu4Ac4D53/02CcqcDjwND3L3IzHoA7wDLoyKvufslNa20iNSumrTW60tw\nb9Ik1CXWKi+rVSs49lgYNQry80PwLykJ+3jQQSH10qlTCOqZpNLdMbMcYCowBlgHzDez2e6+rEy5\nVsCPgNfLbOJ9dx+YovqKSA1lQms9Nze0qvfuLX95hw4haB9yCHTpAocfHh5du+6fP6/PLfLaksx3\n2FBgpbt/AGBmM4HxwLIy5W4EbgF+ltIaikiVxQf2ik6O1vfWeiJ79oTgPXgwDB8eHocfHtIwrVpl\nZzBPVjJBvzOwNm56HTAsvoCZFQBd3f2fZlY26Pc0szeBncC17v7vmlRYJBuVDeKxftzJpGHq48nR\nRo1CPSqrS5MmMHAgHHdcaKV37QqHHRYerVrVTV0zTY2zVWbWCLgDOL+cxRuBbu6+zcwGA0+aWV93\n31lmG5OByQDdauvaY5EGJlEf9dWrw4BekybV/zRMvJyckG7p2XP/7ort2oUWevyjbVvo3DmsI6mV\nTNBfD3SNm+4SzYtpBfQD5poZwMHAbDMb5+5FwF4Ad19gZu8DRwL7XX7g7tOAaQCFhYX18M9VpG4k\nCvRlg3h97LuekxNy7G3awOjRIeXSrx/07x9OjIbwIOmWTNCfD/Qys56EYH82cE5sobsXAx1j02Y2\nF/hp1HunE7Dd3feZ2WFAL+CDFNZfpEFKJi1TXwJ6y5ahV8tnn4Xp3Fw47TQYOTIE9j59lENvSCo9\nVO5eAkwBniV0v5zl7m+b2Q1mNq6S1Y8FlpjZIkJXzkvcfXtNKy1SnyW6C1JsfqLx1iG9gb5Ro5BD\nL+uTT0If9OOPh4cfhq1b4aGHwo0++vZVwG9ozOtLcyJSWFjoRbU9+IRIiiVKy8BX02Xn15XGjcN7\nx4//0r07nHQSDBoUHv37fzWMgDu8/z688EKY168f9O4dhh6Q+svMFrh7YWXlMuyyA5G6UTY9E39j\njdrKv1f25ZGTE/qux7RrF7o0Hnxw6Ld+0EFQWAhHH11xzxczOOKI8JDMo6AvUkZNLl6qLbm5cMop\n4erR2bNh7tyvcuwQWuHHHBOWjxwZrjCN1VcknoK+CPVzCN+cnNBHvWdPePNN+MtfwiM3F/LywmPo\nUBg2LAR5pV8kGQr6kvVmzAgnJWMjJtZ13j3+SyY3Fy64AK6/PgwVEO/jj8NJ1UMP1clTqT4Ffcl4\nyaRr6lpubujT3ro1fOc7cOaZcOKJcMABiddp27ZuxluXzKagLxmjJiNDpkLZYXpjefbly2HTpv2H\nT4DQam/atPxukiK1RUFfGrRkcvG1na5p3z70iGndOvSYOeSQMITAqaeGYQYSadmyduslUh4FfWlw\nkh2qINViefQvvwxdIO+4A849t3bfUyTVFPSlXkp2aODaCPRm4T2/8Y2QZx8zJkyLZAIFfak3KhpV\nsi6GBm7aNLTeL71Ug4NJ5lLHL6lz8WPTdOwYHvHj0UDtBfZmzcIokGYhRdOhQ3jdvTv86U/wwx8q\n4EtmU0tf6kRdn3DNyQljzuzdG+6wdMUV8JOfhK6SItlMQV9qTV2fcFVwF6mcgr7Uitq+yjX2JdK9\n+/5930WkYsrpS42Vl6OfNOmrgF9TXbqEk6vdu3+Vf3/ooRD0V61SwBepCrX0pVqSydHXRLducPPN\nCugiqaaWvlSovLtAxVI3qehp06JFuKI11oJ/+OGwvdWrFfBFaoNa+pJQ2bz86tWhW2VN8/Ndu8Kv\nf62rWUXSQS19+ZpY6768vHx1A358K37NGgV8kXRRS1/2U7Z1X1O5uTBtmlI1IvWFWvoCVNy6T0bs\nKtayV7kq4IvUL2rpZ5FkBzGrKvWVF2k4FPSzRHknZWs6iJlSNyINj9I7WeKaa6qfp2/aFB58MJyI\njb9ASgFfpOFR0M9A5fWtX7OmetuKjT75/e+HAL9qVbiJiK6EFWmYlN7JMKnqW6/UjUhmUks/w5SX\nxkk24Md64Ch1I5K5kgr6ZjbWzJab2Uozu7qCcqebmZtZYdy8X0TrLTezb6ai0pJYsmmc5s3DzbwP\nOihMaxAzkexQaXrHzHKAqcAYYB0w38xmu/uyMuVaAT8CXo+blwecDfQFDgWeN7Mj3X1f6nYhe8V3\nwWzfPsxLplVvlrqLr0SkYUmmpT8UWOnuH7j758BMYHw55W4EbgE+i5s3Hpjp7nvd/UNgZbQ9qaH4\nQc/cw+iWyY5wqZt8i2SvZIJ+Z2Bt3PS6aF4pMysAurr7P6u6brT+ZDMrMrOiLVu2JFXxbFfdLpi5\nueFCKhHJTjU+kWtmjYA7gKuquw13n+buhe5e2KlTp5pWKaPFumPGhjWuzEMPqW+9iHwlmS6b64Gu\ncdNdonkxrYB+wFwL3T8OBmab2bgk1pUkJLphSWW6dw9j6UyaVLv1E5GGI5mgPx/oZWY9CQH7bOCc\n2EJ3LwY6xqbNbC7wU3cvMrNPgUfM7A7CidxewBupq37mq+69ZpXGEZHyVJrecfcSYArwLPAOMMvd\n3zazG6LWfEXrvg3MApYBzwCXqedO1SSbu2/XTqNbikjlzGt6G6QUKyws9KKionRXI+3iUzqV6dYt\n+Ry/iGQmM1vg7oWVldMwDPVQVW5kkpsbbiAuIpIMDcNQj1T1RiZK44hIVamlX08k27pv1gz++Ec4\n77y6qZeIZBYF/TSrSu6+SRPYvBlat679eolIZlLQT6Oq5O6bNIF77lHAF5GaUU4/jZLtjtmtG0yf\nDhdcUPt1EpHMppZ+GlU2DHLz5nDvvTpRKyKpo5Z+GsR66VR0iUT37gr4IpJ6aunXscry+LpNoYjU\nJrX060gyffDV715Eapta+nUgmV46ZuE2hSIitUkt/TqQTC8d3c1KROqCgn4dqKyXjoZBFpG6oqBf\ni5LtpaM8vojUFeX0a4l66YhIfaSWfi2pKI+v1r2IpIta+rUk0QBq6qUjIumkln4KzZgRWvHh/vDl\nUy8dEUkntfRTJJm++OqlIyLpppZ+iiTK4efk6GblIlJ/qKWfIoly+F9+GR4iIvWBWvop0rZt+fOV\nwxeR+kRBv4ZmzIBDD4WPP/76MuXwRaS+UdCvgdjJ240bv5oX67mjHL6I1EfK6ddAeSdv3UPAV198\nEamP1NKvhtiYOolO3lY2wJqISLqopV9FyfTH18lbEamvkmrpm9lYM1tuZivN7Opyll9iZkvNbJGZ\nvWxmedH8Hmb2aTR/kZn9MdU7UNcqGxtfJ29FpD6rtKVvZjnAVGAMsA6Yb2az3X1ZXLFH3P2PUflx\nwB3A2GjZ++4+MLXVTp+KUjfdu4eAr5O3IlJfJdPSHwqsdPcP3P1zYCYwPr6Au++Mm2wBVDCCfMNU\n2dj4sZO3CvgiUp8lk9PvDKyNm14HDCtbyMwuA34CHACcELeop5m9CewErnX3f5ez7mRgMkC3epgQ\nT2ZsfKV0RKQhSFnvHXef6u6HA/8JXBvN3gh0c/dBhC+ER8ysdTnrTnP3Qncv7NSpU6qqlDIaG19E\nMkUyLf31QNe46S7RvERmAncDuPteYG/0eoGZvQ8cCRRVq7ZpkiiPr7HxRaShSaalPx/oZWY9zewA\n4GxgdnwBM+sVN3kKsCKa3yk6EYyZHQb0Aj5IRcXr0qGHlj+/HmaiREQqVGlL391LzGwK8CyQA9zv\n7m+b2Q1AkbvPBqaY2WjgC2AHcF60+rHADWb2BfAlcIm7b6+NHaktU6fC+nJ+1yiPLyINkXmi7ihp\nUlhY6EVF9SP7s3dvaM0feSScdhrcdVdI9XTrpq6ZIlK/mNkCdy+srJyuyK3AY4/BRx/Bww/DmDFw\n5ZXprpGISM1o7J0E3EPLPi8PRo9Od21ERFJDLf0EXnkFFi6EP/6x4hudi4g0JGrpJ3DXXdCiRcjd\nN2oUrsadMSPdtRIRqRm19MuxZg387W8h2K+NrkVevTpclQs6gSsiDZda+mXs2QNXXRVuZl5S8vVl\n11yTnnqJiKSCWvpxFi2CCRPg3XcTl9ENUkSkIVNLP/LoozBsGGzaBAcemLicrsIVkYZMQT9yyy1w\n0EHhgqyPPiq/jK7CFZGGTkGfkKt/6y0oLoZPPy2/jEbTFJFMoJw+8OabsG8f7NxZ/nKNpikimUIt\nfWD+/PDcuXP5y5XHF5FMoaBPCPqdO4e8fm7u/suUxxeRTKKgD7zxBgwZEvL106aF/L2Z8vgiknmy\nPqe/YwesXAkXXhimJ05UkBeRzJX1Lf3Y0P1DhqS3HiIidSHrg37sJG5hpbceEBFp+BT050OvXtC2\nbbprIiJS+7I+6L/xBgwdmu5aiIjUjawO+hs2hIfy+SKSLbI66Mfy+Qr6IpItsj7o5+TAwIHpromI\nSN3I6qD/xhvQvz888US4HaJuiygimS5rg/6+ffDaa9C+fbgN4urV4P7VbREV+EUkE2Vt0H/rLdi1\nCxYvDkMrx9NtEUUkU2Vt0H/55fC8bVv5y3VbRBHJRFkb9F95JYysmWjYZA2nLCKZKKmgb2ZjzWy5\nma00s6vLWX6JmS01s0Vm9rKZ5cUt+0W03nIz+2YqK18TL78Mw4fDzTdrOGURyR6VBn0zywGmAicD\necCE+KAeecTd+7v7QOC3wB3RunnA2UBfYCzwf6PtpdWaNbB2LYwYoeGURSS7JNPSHwqsdPcP3P1z\nYCYwPr6Au8ffaLAF4NHr8cBMd9/r7h8CK6PtpdUrr4Tnm28O3TSvuSa07L/8MtwWUQFfRDJVMuPp\ndwbWxk2vA4aVLWRmlwE/AQ4ATohb97Uy637tpoRmNhmYDNCtDpLp06eH502bwnOsmyYo4ItIZkvZ\niVx3n+ruhwP/CVxbxXWnuXuhuxd26tQpVVVKaO7cr89TN00RyQbJBP31QNe46S7RvERmAt+p5rq1\nrrgYvvii/GXqpikimS6ZoD8f6GVmPc3sAMKJ2dnxBcysV9zkKcCK6PVs4Gwza2pmPYFewBs1r3b1\nvfZa4mXqpikima7SnL67l5jZFOBZIAe4393fNrMbgCJ3nw1MMbPRwBfADuC8aN23zWwWsAwoAS5z\n9321tC9JeeWVcPK2aVP49NOv5qubpohkA3P3ykvVocLCQi+K3bg2xWbMgIsugr17oUOHMG/79tDC\nv+kmncQVkYbLzBa4e6U3fk2m905GiA/4EIZfyM2Fhx5SsBeR7JE1wzD84hdfBfwY9dgRkWyTNUF/\n7dry56vHjohkk6wI+osWJV6mHjsikk2yIuj/8IfQujU0b77/fPXYEZFsk/FBf8MG+N//hWuvhXvv\n1cBqIpLdMr73zksvhecTToDBgxXkRSS7ZXxLf+7ckNoZODDdNRERSb+MD/ovvQQjR0JO2kfxFxFJ\nv4wO+hs3wvLlcPzx6a6JiEj9kNFBf9688Hzccemth4hIfZHRQX/uXGjVCgYNSndNRETqh4wP+iNG\nQOOM76MkIpKcjA36mzfDu+8qny8iEi9jg36sf77y+SIiX8nooN+yJRQUpLsmIiL1R8YG/blzYfhw\nmDULevQId8vq0SOMqy8ikq0yMugXF8OyZeFK3MmTYfVqcA/Pkycr8ItI9srIoL90aXh+6aVwo5R4\nunGKiGSzjAz6ixeH548+Kn+5bpwiItkqI4P+kiXQrl3iG6Toxikikq0yNugPGAA33xxulBJPN04R\nkWyWcUH/yy9DTj8/P4ydP22abpwiIhKTcQMUfPgh7N4dWvoQAryCvIhIkHEt/dhJ3FjQFxGRr2Rc\n0F+yJKRy+vZNd01EROqfpIK+mY01s+VmttLMri5n+U/MbJmZLTGzOWbWPW7ZPjNbFD1mp7Ly5Vmy\nBA46CPLydBWuiEhZleb0zSwHmAqMAdYB881strsviyv2JlDo7nvM7FLgt8BZ0bJP3b3O7lD7yiuw\nbRvs2xemY1fhgnL7IiLJtPSHAivd/QN3/xyYCYyPL+DuL7p77NrX14Auqa1mcj75JFyQFQv4MboK\nV0QkSCbodwbWxk2vi+YlchHwdNx0MzMrMrPXzOw71ahj0t56K/EyXYUrIpLiLptmNgkoBOJHse/u\n7uvN7DDgBTNb6u7vl1lvMjAZoFsNLpeN9dwpj67CFRFJrqW/HugaN90lmrcfMxsNXAOMc/e9sfnu\nvj56/gCYC3ztjrXuPs3dC929sFOnTlXagXhLlkCzZtC8+f7zdRWuiEiQTNCfD/Qys55mdgBwNrBf\nLxwzGwTcQwj4H8XNb2dmTaPXHYHhQPwJ4JRasgQGD4Z779VVuCIi5ak0vePuJWY2BXgWyAHud/e3\nzewGoMjdZwO3Ai2Bv5gZwBp3Hwf0Ae4xsy8JXzC/KdPrJ2XcQ9CfNElX4YqIJJJUTt/d/wX8q8y8\nX8W9Hp1gvVeB/jWpYLLWroWdO3UlrohIRTJm7J1u3WDTJmjaNN01ERGpvzIm6EO4EldERBLLuLF3\nREQkMQV9EZEsoqAvIpJFFPRFRLKIgr6ISBZR0BcRySIK+iIiWURBX0Qki2TUxVkimeaLL75g3bp1\nfPbZZ+muitQTzZo1o0uXLjRp0qRa6yvoi9Rj69ato1WrVvTo0YNoMEPJYu7Otm3bWLduHT179qzW\nNpTeEanHPvvsMzp06KCALwCYGR06dKjRLz8FfZF6TgFf4tX070FBX0Qkiyjoi2SQGTOgRw9o1Cg8\nz5hRs+1t27aNgQMHMnDgQA4++GA6d+5cOv35558ntY0LLriA5cuXV1hm6tSpzKhpZSUpOpErkiFm\nzIDJk2HPnjC9enWYhurfSa5Dhw4sWrQIgOuvv56WLVvy05/+dL8y7o6706hR+W3I6dOnV/o+l112\nWfUqmEYlJSU0btzwQqha+iIZ4pprvgr4MXv2hPmptnLlSvLy8pg4cSJ9+/Zl48aNTJ48mcLCQvr2\n7csNN9xQWnbEiBEsWrSIkpIS2rZty9VXX01+fj7HHHMMH30Ubql97bXXcuedd5aWv/rqqxk6dChH\nHXUUr776KgC7d+/m9NNPJy8vjzPOOIPCwsLSL6R41113HUOGDKFfv35ccskluDsA7733HieccAL5\n+fkUFBSwatUqAG6++Wb69+9Pfn4+10QfVqzOAJs2beKII44A4L777uM73/kOo0aN4pvf/CY7d+7k\nhBNOoKCggAEDBvCPf/yjtB7Tp09nwIAB5Ofnc8EFF1BcXMxhhx1GSUkJADt27Nhvuq4o6ItkiDVr\nqja/pt59912uvPJKli1bRufOnfnNb35DUVERixcv5rnnnmPZsq/fDru4uJjjjjuOxYsXc8wxx3D/\n/feXu21354033uDWW28t/QL5wx/+wMEHH8yyZcv4r//6L958881y1/3Rj37E/PnzWbp0KcXFxTzz\nzDMATJgwgSuvvJLFixfz6quvcuCBB/LUU0/x9NNP88Ybb7B48WKuuuqqSvf7zTff5G9/+xtz5syh\nefPmPPnkkyxcuJDnn3+eK6+8EoDFixdzyy23MHfuXBYvXsztt99OmzZtGD58eGl9Hn30Uc4888w6\n/7WgoC+SIbp1q9r8mjr88MMpLCwsnX700UcpKCigoKCAd955p9yg37x5c04++WQABg8eXNraLuu0\n0077WpmXX36Zs88+G4D8/Hz69u1b7rpz5sxh6NCh5Ofn89JLL/H222+zY8cOtm7dyre//W0gXOCU\nm5vL888/z4UXXkjz5s0BaN++faX7fdJJJ9GuXTsgfDldffXVDBgwgJNOOom1a9eydetWXnjhBc46\n66zS7cWeL7744tJ01/Tp07ngggsqfb9UU9AXyRA33QS5ufvPy80N82tDixYtSl+vWLGCu+66ixde\neIElS5YwduzYcvuSH3DAAaWvc3JyEqY2mkY3u66oTHn27NnDlClTeOKJJ1iyZAkXXnhhtfq0N27c\nmC+//BLga+vH7/eDDz5IcXE8k+DaAAAN7klEQVQxCxcuZNGiRXTs2LHC9zvuuON47733ePHFF2nS\npAm9e/euct1qSkFfJENMnAjTpkH37mAWnqdNq/5J3KrYuXMnrVq1onXr1mzcuJFnn3025e8xfPhw\nZs2aBcDSpUvL/SXx6aef0qhRIzp27MiuXbv461//CkC7du3o1KkTTz31FBAC+Z49exgzZgz3338/\nn376KQDbt28HoEePHixYsACAxx9/PGGdiouLOfDAA2ncuDHPPfcc69evB+CEE07gscceK91e7Blg\n0qRJTJw4MS2tfFDQF8koEyfCqlXw5ZfhuS4CPkBBQQF5eXn07t2bc889l+HDh6f8PS6//HLWr19P\nXl4e//3f/01eXh5t2rTZr0yHDh0477zzyMvL4+STT2bYsGGly2bMmMHtt9/OgAEDGDFiBFu2bOHU\nU09l7NixFBYWMnDgQH73u98B8LOf/Yy77rqLgoICduzYkbBO3//+93n11Vfp378/M2fOpFevXkBI\nP/385z/n2GOPZeDAgfzsZz8rXWfixIkUFxdz1llnpfLjSZrFzmzXF4WFhV5UVJTuaojUC++88w59\n+vRJdzXqhZKSEkpKSmjWrBkrVqzgpJNOYsWKFQ2u2+TMmTN59tlnk+rKmkh5fxdmtsDdCxOsUqph\nfVoikrU++eQTTjzxREpKSnB37rnnngYX8C+99FKef/750h486dCwPjERyVpt27YtzbM3VHfffXe6\nq6CcvohINkkq6JvZWDNbbmYrzezqcpb/xMyWmdkSM5tjZt3jlp1nZiuix3mprLyIiFRNpUHfzHKA\nqcDJQB4wwczyyhR7Eyh09wHA48Bvo3XbA9cBw4ChwHVm1i511RcRkapIpqU/FFjp7h+4++fATGB8\nfAF3f9HdY6N+vAZ0iV5/E3jO3be7+w7gOWBsaqouIiJVlUzQ7wysjZteF81L5CLg6aqsa2aTzazI\nzIq2bNmSRJVEpC6MGjXqaxda3XnnnVx66aUVrteyZUsANmzYwBlnnFFumeOPP57Kumffeeed7Ikb\nRe5b3/oWH3/8cTJVlwRSeiLXzCYBhcCtVVnP3ae5e6G7F3bq1CmVVRKRGpgwYQIzZ87cb97MmTOZ\nMGFCUusfeuihFV7RWpmyQf9f//oXbdu2rfb26pq7lw7nUF8kE/TXA13jprtE8/ZjZqOBa4Bx7r63\nKuuKSOV+/GM4/vjUPn7844rf84wzzuCf//xn6Q1TVq1axYYNGxg5cmRpv/mCggL69+/P3//+96+t\nv2rVKvr16weEIRLOPvts+vTpw3e/+93SoQ8g9F+PDct83XXXAfD73/+eDRs2MGrUKEaNGgWE4RG2\nbt0KwB133EG/fv3o169f6bDMq1atok+fPvzgBz+gb9++nHTSSfu9T8xTTz3FsGHDGDRoEKNHj2bz\n5s1AuBbgggsuoH///gwYMKB0GIdnnnmGgoIC8vPzOfHEE4Fwf4HbbrutdJv9+vVj1apVrFq1iqOO\nOopzzz2Xfv36sXbt2nL3D2D+/Pl84xvfID8/n6FDh7Jr1y6OPfbY/YaMHjFiBIsXL674QFVBMv30\n5wO9zKwnIWCfDZwTX8DMBgH3AGPd/aO4Rc8CN8edvD0J+EWNay0idaJ9+/YMHTqUp59+mvHjxzNz\n5ky+973vYWY0a9aMJ554gtatW7N161aOPvpoxo0bl/AernfffTe5ubm88847LFmyhIKCgtJlN910\nE+3bt2ffvn2ceOKJLFmyhCuuuII77riDF198kY4dO+63rQULFjB9+nRef/113J1hw4Zx3HHH0a5d\nO1asWMGjjz7Kvffey/e+9z3++te/MmnSpP3WHzFiBK+99hpmxn333cdvf/tbbr/9dm688UbatGnD\n0qVLgTDm/ZYtW/jBD37AvHnz6Nmz537j6CSyYsUKHnjgAY4++uiE+9e7d2/OOussHnvsMYYMGcLO\nnTtp3rw5F110EX/+85+58847ee+99/jss8/Iz8+v0nGrSKVB391LzGwKIYDnAPe7+9tmdgNQ5O6z\nCemclsBfogO+xt3Huft2M7uR8MUBcIO7V/6JicjXRI3ZOhdL8cSC/p/+9CcgpC5++ctfMm/ePBo1\nasT69evZvHkzBx98cLnbmTdvHldccQUAAwYMYMCAAaXLZs2axbRp0ygpKWHjxo0sW7Zsv+Vlvfzy\ny3z3u98tHfHytNNO49///jfjxo2jZ8+eDBw4EEg8fPO6des466yz2LhxI59//jk9e/YE4Pnnn98v\nndWuXTueeuopjj322NIyyQy/3L1799KAn2j/zIxDDjmEIUOGANC6dWsAzjzzTG688UZuvfVW7r//\nfs4///xK368qksrpu/u/3P1Idz/c3W+K5v0qCvi4+2h3P8jdB0aPcXHr3u/uR0SP6g82UYlU3xtU\nRILx48czZ84cFi5cyJ49exg8eDAQBjDbsmULCxYsYNGiRRx00EHVGsb4ww8/5LbbbmPOnDksWbKE\nU045pVrbiYkNywyJh2a+/PLLmTJlCkuXLuWee+6p8fDLsP8QzPHDL1d1/3JzcxkzZgx///vfmTVr\nFhNTPGpeRlyRG7s36OrV4P7VvUEV+EVqrmXLlowaNYoLL7xwvxO4sWGFmzRpwosvvsjq1asr3M6x\nxx7LI488AsBbb73FkiVLgDAsc4sWLWjTpg2bN2/m6aefLl2nVatW7Nq162vbGjlyJE8++SR79uxh\n9+7dPPHEE4wcOTLpfSouLqZz59CR8IEHHiidP2bMGKZOnVo6vWPHDo4++mjmzZvHhx9+COw//PLC\nhQsBWLhwYenyshLt31FHHcXGjRuZPz8kQnbt2lX6BXXxxRdzxRVXMGTIkNIbtqRKRgT9urw3qEg2\nmjBhAosXL94v6E+cOJGioiL69+/Pgw8+WOkNQS699FI++eQT+vTpw69+9avSXwz5+fkMGjSI3r17\nc8455+w3LPPkyZMZO3Zs6YncmIKCAs4//3yGDh3KsGHDuPjiixk0aFDS+3P99ddz5plnMnjw4P3O\nF1x77bXs2LGDfv36kZ+fz4svvkinTp2YNm0ap512Gvn5+aVDIp9++uls376dvn378j//8z8ceeSR\n5b5Xov074IADeOyxx7j88svJz89nzJgxpb8ABg8eTOvWrWtlzP2MGFq5UaPQwi/LLIwrLtJQaWjl\n7LRhwwaOP/543n33XRo1+nrbvCZDK2dES7+u7w0qIlJbHnzwQYYNG8ZNN91UbsCvqYwI+nV9b1AR\nkdpy7rnnsnbtWs4888xa2X5GBP103htUpLbVtxSspFdN/x4y5iYqEycqyEvmadasGdu2baNDhw4J\nL3qS7OHubNu2jWbNmlV7GxkT9EUyUZcuXVi3bh0aiFBimjVrRpcuXSovmICCvkg91qRJk9IrQUVS\nISNy+iIikhwFfRGRLKKgLyKSRerdFblmtgWoeBCPinUEtqaoOg1FNu4zZOd+Z+M+Q3bud1X3ubu7\nV3oXqnoX9GvKzIqSuRQ5k2TjPkN27nc27jNk537X1j4rvSMikkUU9EVEskgmBv1p6a5AGmTjPkN2\n7nc27jNk537Xyj5nXE5fREQSy8SWvoiIJKCgLyKSRTIm6JvZWDNbbmYrzezqdNentphZVzN70cyW\nmdnbZvajaH57M3vOzFZEz6m9sWY9YGY5Zvammf0jmu5pZq9Hx/wxMzsg3XVMNTNra2aPm9m7ZvaO\nmR2T6cfazK6M/rbfMrNHzaxZJh5rM7vfzD4ys7fi5pV7bC34fbT/S8ysoLrvmxFB38xygKnAyUAe\nMMHM8tJbq1pTAlzl7nnA0cBl0b5eDcxx917AnGg60/wIeCdu+hbgd+5+BLADuCgttapddwHPuHtv\nIJ+w/xl7rM2sM3AFUOju/YAc4Gwy81j/GRhbZl6iY3sy0Ct6TAburu6bZkTQB4YCK939A3f/HJgJ\njE9znWqFu29094XR612EINCZsL8PRMUeAL6TnhrWDjPrApwC3BdNG3AC8HhUJBP3uQ1wLPAnAHf/\n3N0/JsOPNWH03+Zm1hjIBTaSgcfa3ecB28vMTnRsxwMPevAa0NbMDqnO+2ZK0O8MrI2bXhfNy2hm\n1gMYBLwOHOTuG6NFm4CD0lSt2nIn8HMgdqv7DsDH7l4STWfiMe8JbAGmR2mt+8ysBRl8rN19PXAb\nsIYQ7IuBBWT+sY5JdGxTFuMyJehnHTNrCfwV+LG774xf5qEfbsb0xTWzU4GP3H1BuutSxxoDBcDd\n7j4I2E2ZVE4GHut2hFZtT+BQoAVfT4Fkhdo6tpkS9NcDXeOmu0TzMpKZNSEE/Bnu/rdo9ubYz73o\n+aN01a8WDAfGmdkqQuruBEKuu22UAoDMPObrgHXu/no0/TjhSyCTj/Vo4EN33+LuXwB/Ixz/TD/W\nMYmObcpiXKYE/flAr+gM/wGEEz+z01ynWhHlsv8EvOPud8Qtmg2cF70+D/h7Xdettrj7L9y9i7v3\nIBzbF9x9IvAicEZULKP2GcDdNwFrzeyoaNaJwDIy+FgT0jpHm1lu9Lce2+eMPtZxEh3b2cC5US+e\no4HiuDRQ1bh7RjyAbwHvAe8D16S7PrW4nyMIP/mWAIuix7cIOe45wArgeaB9uutaS/t/PPCP6PVh\nwBvASuAvQNN0168W9ncgUBQd7yeBdpl+rIH/Bt4F3gIeAppm4rEGHiWct/iC8KvuokTHFjBCD8X3\ngaWE3k3Vel8NwyAikkUyJb0jIiJJUNAXEckiCvoiIllEQV9EJIso6IuIZBEFfRGRLKKgLyKSRf4/\n1suad2nvKikAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XmYVNW57/HvyyyDjB2NgDTGXKEZ\nBKwguUgY5HpQox4MMWKDQzSI1xM1ao5EjDEmeNAYRYxD0DiCEK9oNA7h5EQS4olBGw6CigajoC0o\nDcokmtDw3j/2rrZoauruqq7p93meeqjae9Xea9em31r1rrXXNndHRESKS4tcV0BERDJPwV1EpAgp\nuIuIFCEFdxGRIqTgLiJShBTcRUSKkIK7xGVmLc1sl5kdnsmyuWRmR5pZxsf+mtl4M1sf8/pNMxuV\nTtlG7OteM7u6se9Pst2fmtkDmd6u5E6rXFdAMsPMdsW8bA/8A9gbvr7Q3Rc0ZHvuvhfomOmypcDd\nj8rEdszsAmCKu4+J2fYFmdi2FD8F9yLh7nXBNWwZXuDu/5WovJm1cvfa5qibiDQ/pWVKRPiz+9dm\nttDMdgJTzOyrZvZXM9tmZpvMbK6ZtQ7LtzIzN7Py8PX8cP1zZrbTzF40s74NLRuuP9HM/mZm283s\ndjP7bzM7N0G906njhWb2lpl9bGZzY97b0sxuNbOtZvY2MCHJ5zPTzBbVW3aHmd0SPr/AzNaGx/P3\nsFWdaFvVZjYmfN7ezB4O6/YacEy9steY2dvhdl8zs1PD5YOAXwCjwpTXlpjP9rqY908Pj32rmf3G\nzL6YzmeTiplNDOuzzcyeN7OjYtZdbWYbzWyHmb0Rc6wjzGxluPxDM/tZuvuTLHB3PYrsAawHxtdb\n9lPgn8ApBF/qBwFfAY4l+AV3BPA34N/C8q0AB8rD1/OBLUAEaA38GpjfiLJfAHYCp4XrLgf2AOcm\nOJZ06vgk0BkoBz6KHjvwb8BrQC+gO7As+C8fdz9HALuADjHb3gxEwtenhGUMGAd8CgwO140H1sds\nqxoYEz6/Gfgj0BXoA7xer+wZwBfDc3JWWIdDwnUXAH+sV8/5wHXh8xPCOg4B2gF3As+n89nEOf6f\nAg+Ez/uH9RgXnqOrgTfD5wOADcChYdm+wBHh85eByeHzTsCxuf5bKOWHWu6l5QV3/62773P3T939\nZXdf7u617v42MA8YneT9j7l7lbvvARYQBJWGlv06sMrdnwzX3UrwRRBXmnX8D3ff7u7rCQJpdF9n\nALe6e7W7bwVmJ9nP28CrBF86AP8H+Njdq8L1v3X3tz3wPPAHIG6naT1nAD9194/dfQNBazx2v4+6\n+6bwnDxC8MUcSWO7AJXAve6+yt0/A2YAo82sV0yZRJ9NMmcCT7n78+E5mk3wBXEsUEvwRTIgTO29\nE352EHxJf9nMurv7TndfnuZxSBYouJeW92JfmFk/M3vGzD4wsx3A9UCPJO//IOb5bpJ3oiYqe1hs\nPdzdCVq6caVZx7T2RdDiTOYRYHL4/KzwdbQeXzez5Wb2kZltI2g1J/usor6YrA5mdq6ZvRKmP7YB\n/dLcLgTHV7c9d98BfAz0jCnTkHOWaLv7CM5RT3d/E7iC4DxsDtN8h4ZFzwMqgDfN7CUzOynN45As\nUHAvLfWHAf6SoLV6pLsfDFxLkHbIpk0EaRIAzMzYPxjV15Q6bgJ6x7xONVTzUWC8mfUkaME/Etbx\nIOAx4D8IUiZdgP9Msx4fJKqDmR0B3AVcBHQPt/tGzHZTDdvcSJDqiW6vE0H65/006tWQ7bYgOGfv\nA7j7fHcfSZCSaUnwueDub7r7mQSpt58Di82sXRPrIo2k4F7aOgHbgU/MrD9wYTPs82lgmJmdYmat\ngEuBsizV8VHgMjPraWbdgauSFXb3D4AXgAeAN919XbiqLdAGqAH2mtnXgeMbUIerzayLBdcB/FvM\nuo4EAbyG4HvuOwQt96gPgV7RDuQ4FgLnm9lgM2tLEGT/7O4Jfwk1oM6nmtmYcN/fJ+gnWW5m/c1s\nbLi/T8PHPoIDmGpmPcKW/vbw2PY1sS7SSArupe0K4ByCP9xfEnR8ZpW7fwh8C7gF2Ap8CfgfgnH5\nma7jXQS58TUEnX2PpfGeRwg6SOtSMu6+Dfge8ARBp+Qkgi+pdPyI4BfEeuA54KGY7a4GbgdeCssc\nBcTmqX8PrAM+NLPY9Er0/b8jSI88Eb7/cII8fJO4+2sEn/ldBF88E4BTw/x7W+Amgn6SDwh+KcwM\n33oSsNaC0Vg3A99y9382tT7SOBakPEVyw8xaEqQBJrn7n3NdH5FioZa7NDszmxCmKdoCPyQYZfFS\njqslUlQU3CUXjgPeJvjJ/y/ARHdPlJYRkUZQWkZEpAip5S4iUoRyNnFYjx49vLy8PFe7FxEpSCtW\nrNji7smGDwM5DO7l5eVUVVXlavciIgXJzFJdaQ0oLSMiUpQU3EVEilDK4G5mvc1sqZm9Hs7vfGmc\nMqeZ2WozW2VmVWZ2XHaqKyIi6Ugn514LXOHuK8OJiVaY2e/d/fWYMn8gmCLUzWwwwdwU/eJtTERy\nY8+ePVRXV/PZZ5/luiqShnbt2tGrVy9at040tVByKYO7u28imLcCd99pZmsJZvF7PaZM7P07O5B6\nNjsRaWbV1dV06tSJ8vJygsk4JV+5O1u3bqW6upq+ffumfkMcDcq5W3AbtaHsP7lRdN1EM3sDeAb4\ndoL3TwvTNlU1NTUNruyCBVBeDi1aBP8uaNAtn0VK22effUb37t0V2AuAmdG9e/cm/cpKO7ibWUdg\nMXBZeFOA/bj7E+7eD/hX4CfxtuHu89w94u6RsrKUwzT3s2ABTJsGGzaAe/DvtGkK8CINocBeOJp6\nrtIK7uGczouBBe7+eLKy7r4MOMLM0r2bTFpmzoTdu/dftnt3sFxERPaXzmgZA34FrHX3WxKUOTIs\nh5kNI5jzeWsmK/ruuw1bLiL5ZevWrQwZMoQhQ4Zw6KGH0rNnz7rX//xnetO+n3feebz55ptJy9xx\nxx0syNBP+uOOO45Vq1ZlZFvNLZ3RMiOBqcAaM4se5dWEtwtz97uBbwBnm9kegjuzfMszPCPZ4YcH\nqZh4y0Uk8xYsCH4Zv/tu8Hc2axZUNuFWIN27d68LlNdddx0dO3bkyiuv3K+Mu+PutGgRv915//33\np9zPxRdf3PhKFpGULXd3f8Hdzd0Hu/uQ8PGsu98dBnbc/UZ3HxCu+6q7v5Dpis6aBe3b77+sfftg\nuYhkVnP2cb311ltUVFRQWVnJgAED2LRpE9OmTSMSiTBgwACuv/76urLRlnRtbS1dunRhxowZHH30\n0Xz1q19l8+bNAFxzzTXMmTOnrvyMGTMYPnw4Rx11FH/5y18A+OSTT/jGN75BRUUFkyZNIhKJpGyh\nz58/n0GDBjFw4ECuvvpqAGpra5k6dWrd8rlz5wJw6623UlFRweDBg5kyZUrGP7N05GxumYaKthj+\n/d9h48agJXHDDU1rSYhIfMn6uLLxN/fGG2/w0EMPEYlEAJg9ezbdunWjtraWsWPHMmnSJCoqKvZ7\nz/bt2xk9ejSzZ8/m8ssv57777mPGjBkHbNvdeemll3jqqae4/vrr+d3vfsftt9/OoYceyuLFi3nl\nlVcYNmxY0vpVV1dzzTXXUFVVRefOnRk/fjxPP/00ZWVlbNmyhTVr1gCwbds2AG666SY2bNhAmzZt\n6pY1t4KafqCyEu68M3g+Z44Cu0i2NHcf15e+9KW6wA6wcOFChg0bxrBhw1i7di2vv/76Ae856KCD\nOPHEEwE45phjWL9+fdxtn3766QeUeeGFFzjzzDMBOProoxkwYEDS+i1fvpxx48bRo0cPWrduzVln\nncWyZcs48sgjefPNN7nkkktYsmQJnTt3BmDAgAFMmTKFBQsWNPoipKYqqOAOcPLJ0Ls3XHutxryL\nZEuivqxs9XF16NCh7vm6deu47bbbeP7551m9ejUTJkyIO967TZs2dc9btmxJbW1t3G23bds2ZZnG\n6t69O6tXr2bUqFHccccdXHjhhQAsWbKE6dOn8/LLLzN8+HD27t2b0f2mo+CCe6tWMGIEvPqqxryL\nZEsu+7h27NhBp06dOPjgg9m0aRNLlizJ+D5GjhzJo48+CsCaNWvi/jKIdeyxx7J06VK2bt1KbW0t\nixYtYvTo0dTU1ODufPOb3+T6669n5cqV7N27l+rqasaNG8dNN93Eli1b2F0/x9UMCibnHuvFFw9c\nls18oEipif4dZXK0TLqGDRtGRUUF/fr1o0+fPowcOTLj+/jud7/L2WefTUVFRd0jmlKJp1evXvzk\nJz9hzJgxuDunnHIKJ598MitXruT888/H3TEzbrzxRmpraznrrLPYuXMn+/bt48orr6RTp04ZP4ZU\ncnYP1Ugk4o29WUeLFkGLvT4z2LeviRUTKVJr166lf//+ua5GXqitraW2tpZ27dqxbt06TjjhBNat\nW0erVvnV3o13zsxshbtHErylTn4dSZo05l1EmmLXrl0cf/zx1NbW4u788pe/zLvA3lQFeTSzZsF3\nvgOffvr5Mo15F5F0denShRUrVuS6GllVcB2qEOT97rkHunULXh96KMybp3y7iEhUQQZ3CAL5+vXQ\nqROMGxcs09BIEZFAQaZlojp1gm9/G37xC3j8cYgOhY0OjQS15kWkNBVsyz3qu9+FvXs/D+xRmg5Y\nREpZwQf3L30p8TpNByySP8aOHXvABUlz5szhoosuSvq+jh07ArBx40YmTZoUt8yYMWNINbR6zpw5\n+11MdNJJJ2Vk3pfrrruOm2++ucnbybSCD+4AX/hC/OUaGimSPyZPnsyiRYv2W7Zo0SImT56c1vsP\nO+wwHnvssUbvv35wf/bZZ+nSpUujt5fviiK4//znwQVMsTQ0UiS/TJo0iWeeeabuxhzr169n48aN\njBo1qm7c+bBhwxg0aBBPPvnkAe9fv349AwcOBODTTz/lzDPPpH///kycOJFPY8ZFX3TRRXXTBf/o\nRz8CYO7cuWzcuJGxY8cyduxYAMrLy9myZQsAt9xyCwMHDmTgwIF10wWvX7+e/v37853vfIcBAwZw\nwgkn7LefeFatWsWIESMYPHgwEydO5OOPP67bf3QK4OiEZX/605/qblYydOhQdu7c2ejPNp6C7lCN\nmjIFli6F++4LXvfp03yXSosUossug0zfYGjIkGC21kS6devG8OHDee655zjttNNYtGgRZ5xxBmZG\nu3bteOKJJzj44IPZsmULI0aM4NRTT014H9G77rqL9u3bs3btWlavXr3flL2zZs2iW7du7N27l+OP\nP57Vq1dzySWXcMstt7B06VJ69Nj/DqArVqzg/vvvZ/ny5bg7xx57LKNHj6Zr166sW7eOhQsXcs89\n93DGGWewePHipPOzn3322dx+++2MHj2aa6+9lh//+MfMmTOH2bNn884779C2bdu6VNDNN9/MHXfc\nwciRI9m1axft2rVrwKedWlG03AFuvx169IBTTgkC+8yZGhYpkm9iUzOxKRl35+qrr2bw4MGMHz+e\n999/nw8//DDhdpYtW1YXZAcPHszgwYPr1j366KMMGzaMoUOH8tprr6WcFOyFF15g4sSJdOjQgY4d\nO3L66afz5z//GYC+ffsyZMgQIPm0whDML79t2zZGjx4NwDnnnMOyZcvq6lhZWcn8+fPrroQdOXIk\nl19+OXPnzmXbtm0Zv0K2KFruEKRhLr4Yfvxj+P3vNSxSJJlkLexsOu200/je977HypUr2b17N8cc\ncwwACxYsoKamhhUrVtC6dWvKy8vjTvObyjvvvMPNN9/Myy+/TNeuXTn33HMbtZ2o6HTBEEwZnCot\nk8gzzzzDsmXL+O1vf8usWbNYs2YNM2bM4OSTT+bZZ59l5MiRLFmyhH79+jW6rvUVTcsdguBupmGR\nIvmqY8eOjB07lm9/+9v7daRu376dL3zhC7Ru3ZqlS5eyId7kUTG+9rWv8cgjjwDw6quvsnr1aiCY\nLrhDhw507tyZDz/8kOeee67uPZ06dYqb1x41ahS/+c1v2L17N5988glPPPEEo0aNavCxde7cma5d\nu9a1+h9++GFGjx7Nvn37eO+99xg7diw33ngj27dvZ9euXfz9739n0KBBXHXVVXzlK1/hjTfeaPA+\nkymaljtAWVn82SJBwyJF8sXkyZOZOHHifiNnKisrOeWUUxg0aBCRSCRlC/aiiy7ivPPOo3///vTv\n37/uF8DRRx/N0KFD6devH717995vuuBp06YxYcIEDjvsMJYuXVq3fNiwYZx77rkMHz4cgAsuuICh\nQ4cmTcEk8uCDDzJ9+nR2797NEUccwf3338/evXuZMmUK27dvx9255JJL6NKlCz/84Q9ZunQpLVq0\nYMCAAXV3lcqUlFP+mllv4CHgEMCBee5+W70ylcBVgAE7gYvc/ZVk223KlL/J9OwZ3GO1vj59gukK\nREqVpvwtPE2Z8jedtEwtcIW7VwAjgIvNrKJemXeA0e4+CPgJMC+tmmfBTTdBy5b7L9OwSBEpNSmD\nu7tvcveV4fOdwFqgZ70yf3H3j8OXfwV6Zbqi6aqshHBoKxC02DVjpIiUmgZ1qJpZOTAUWJ6k2PnA\nc/FWmNk0M6sys6qampqG7LpBfvhDGD8eDjkkCPQaFikSyNWd16Thmnqu0g7uZtYRWAxc5u47EpQZ\nSxDcr4q33t3nuXvE3SNlZWWNqW/aZs6EDz+ECy/UjbRFANq1a8fWrVsV4AuAu7N169YmXdiU1j1U\nzaw18DSwxN1vSVBmMPAEcKK7/y3VNrPVoRrlDgcdBP/4x4Hr1LkqpWjPnj1UV1c3ady3NJ927drR\nq1cvWrduvd/yjN1D1YLrf38FrE0S2A8HHgemphPYm4NZ/MAOGhYppal169b07ds319WQZpLOOPeR\nwFRgjZlFZ6O4GjgcwN3vBq4FugN3hnNB1KbzzZJthx8eP5BrtkgRKXYpg7u7v0Awfj1ZmQuACzJV\nqUy54YbgTk3hJHSAhkWKSGkoqukH6qushHvvhWjKqlu3IA8/dapGzohIcSuq6QfimToV2rSBM8+E\nXbs+b8VrQjERKWZF3XKP+uY3g9Z7bHoGNKGYiBSvkgjuLVrAnj3x12nkjIgUo5II7pB4hIxGzohI\nMSqZ4H7DDUHuPZZGzohIsSqZ4K6RMyJSSkomuEMQyKM3Vd+5E7Zu1ZwzIlKcSiq4A0yYAG3bHtjB\nqpEzIlJMSi64a84ZESkFJRfcIZgVMh6NnBGRYlGSwX3WLKg/TbJGzohIMSnJ4B4dOdOxY/C6SxeN\nnBGR4lKSwR2CAP/WW0ELXiNnRKTYlGxwh+Aeq+3awd69+y/XyBkRKXQlHdwBtm2Lv1wjZ0SkkJV8\ncNfIGREpRiUf3GfNCjpTY5kFuXd1ropIoSr54F5ZCffcA4ce+vky9+Bfda6KSKEq+eAOQYDftOnz\noZGx1LkqIoUoZXA3s95mttTMXjez18zs0jhl+pnZi2b2DzO7MjtVzb5du+IvV+eqiBSadFrutcAV\n7l4BjAAuNrOKemU+Ai4Bbs5w/ZqVOldFpFikDO7uvsndV4bPdwJrgZ71ymx295eBBDezKwzqXBWR\nYtGgnLuZlQNDgeXZqEyuRTtXDznk82XqXBWRQpR2cDezjsBi4DJ339GYnZnZNDOrMrOqmpqaxmwi\n6yor4YMPoEOHA9epc1VECkVawd3MWhME9gXu/nhjd+bu89w94u6RsrKyxm6mWXzySfzl6lwVkUKQ\nzmgZA34FrHX3W7JfpfygzlURKWTptNxHAlOBcWa2KnycZGbTzWw6gJkdambVwOXANWZWbWYHZ7He\nWafOVREpZK1SFXD3FwBLUeYDoFemKpUPKiuDf7///eACJziwczW2nIhIPtEVqklUVsLGjXBwnN8g\n6lwVkXym4J6GHQnGBqlzVUTylYJ7GtS5KiKFRsE9DbNmBTfQjqXOVRHJZwruaaishHnzoHfvz5fp\nylURyWcK7mmqrAxy7LFTE0Spc1VE8o2CewNt3hx/uTpXRSSfKLg3UKJOVHfl30Ukfyi4N1C8ztUo\n5d9FJF8ouDdQtHM10fBI5d9FJB8ouDdCZSWsX594vfLvIpJrCu5NkKj1rvy7iOSagnsTKP8uIvlK\nwb0JlH8XkXyl4N5Eyr+LSD5ScM8QTS4mIvlEwT1DNLmYiOQTBfcMiebfY1vqmlxMRHJFwT2DKiuD\nQB47e2SUOldFpDkpuGdBdXX85Rs2QIsWStOISPYpuGdBsk5Ud6VpRCT7UgZ3M+ttZkvN7HUze83M\nLo1Txsxsrpm9ZWarzWxYdqpbGJJd3BSlNI2IZFM6Lfda4Ap3rwBGABebWUW9MicCXw4f04C7MlrL\nApPq4qYojYEXkWxJGdzdfZO7rwyf7wTWAj3rFTsNeMgDfwW6mNkXM17bAhK9uMkdevWKX0Zz0IhI\ntjQo525m5cBQYHm9VT2B92JeV3PgFwBmNs3MqsysqqampmE1LWCzZ0O7dvHXKf8uItmQdnA3s47A\nYuAyd9/RmJ25+zx3j7h7pKysrDGbKEiVlXDvvZDokJV/F5FMSyu4m1lrgsC+wN0fj1PkfSB2dHev\ncJmEKisT338VlH8XkcxKZ7SMAb8C1rr7LQmKPQWcHY6aGQFsd/dNGaxn0dA9WEWkOaTTch8JTAXG\nmdmq8HGSmU03s+lhmWeBt4G3gHuA/5ud6ha+G26Agw6Kv075dxHJFPPoBCjNLBKJeFVVVU72nWsL\nFsAPfgDvvRd/fZ8+yacRFpHSZWYr3D2SqpyuUM2BysrkOXbNJCkiTaXgnkPJLnJSikZEmkLBPYdS\nTVOgIZIi0lgK7jmUzjQFGiIpIo2h4J5j0WkKEgV4DZEUkcZQcM8TyVI0yr+LSEMpuOeJeLfpi6X8\nu4g0hIJ7Honepi8RDZEUkXQpuOchDZEUkaZScM9DGiIpIk2l4J6H0hkiqRSNiCSj4J6nUg2RBKVo\nRCQxBfc8pxSNiDSGgnueU4pGRBpDwb0AKEUjIg2l4F5AlKIRkXQpuBcQpWhEJF0K7gVGKRoRSYeC\ne4FSikZEklFwL1BK0YhIMimDu5ndZ2abzezVBOu7mtkTZrbazF4ys4GZr6bEoxSNiCSSTsv9AWBC\nkvVXA6vcfTBwNnBbBuolDZBOimbKFLXiRUpJyuDu7suAj5IUqQCeD8u+AZSb2SGZqZ6kI50UDagV\nL1JKMpFzfwU4HcDMhgN9gF7xCprZNDOrMrOqmpqaDOxaotJJ0YA6WkVKRSaC+2ygi5mtAr4L/A+w\nN15Bd5/n7hF3j5SVlWVg11JfqhQNqKNVpBS0auoG3H0HcB6AmRnwDvB2U7crjVNZGfw7c2bquzpN\nm7b/e0SkeDS55W5mXcysTfjyAmBZGPAlR6Ipmvnz1dEqUqpSttzNbCEwBuhhZtXAj4DWAO5+N9Af\neNDMHHgNOD9rtZUGUStepHSZu+dkx5FIxKuqqnKy71JUXp48wEPQGbt+fXPURkQay8xWuHskVTld\noVoi0u1obdFCaRqRYqDgXiLSHQvvrvHwIsVAwb2EpNvRCupsFSl0Cu4lKLYVb5a8rFrxIoVJwb1E\nRVvx+/ald1WrWvEihUXBXdLqbAW14kUKiYK7pN3ZCmrFixQKBXcBGtbZCmrFi+Q7BXfZj1rxIsVB\nwV0OoFa8SOFTcJeE1IoXKVwK7pKUWvEihUnBXdKiVrxIYVFwl7SpFS9SOBTcpcHUihfJfwru0ihq\nxYvkNwV3aRK14kXyk4K7NJla8SL5R8FdMkateJH8oeAuGdWYVvzUqcG88gr0Ipmj4C5Z0ZBWfPQe\n7UrXiGROyuBuZveZ2WYzezXB+s5m9lsze8XMXjOz8zJfTSlEDW3Fg9I1IpmSTsv9AWBCkvUXA6+7\n+9HAGODnZtam6VWTYtGQVnyUWvEiTZMyuLv7MuCjZEWATmZmQMewbG1mqifFQq14keaViZz7L4D+\nwEZgDXCpu++LV9DMpplZlZlV1dTUZGDXUmjqt+JT3aAb1Okq0hiZCO7/AqwCDgOGAL8ws4PjFXT3\nee4ecfdIWVlZBnYthSjaineHhx9Wp6tINmQiuJ8HPO6Bt4B3gH4Z2K6UgMama845B1q0UEteJJFM\nBPd3geMBzOwQ4Cjg7QxsV0pIQztd9+4NWvNK2YjEl85QyIXAi8BRZlZtZueb2XQzmx4W+Qnwv81s\nDfAH4Cp335K9KkuxakwrHpSyEYnHPPqX0cwikYhXVVXlZN+S/xYsgJkzg4Bt9nkAT1efPjBrVvCF\nIVJMzGyFu0dSldMVqpKX4nW6mkHLlum9X+kaKXUK7pL3ooF+3z548MH0UzZK10gpU3CXgtKYcfKg\nC6Kk9Ci4S8FpzDj5KKVrpFQouEtBa8wIm9h0jQK9FCsFdykKjU3XKNBLsVJwl6LRlHQNqANWiouC\nuxSlxl4QFaUOWCl0Cu5S1BqbrolSukYKlYK7FL1E6Rrl5aWYKbhLSVGgl1Kh4C4lK5MdsAr0km8U\n3EVoegdsbKA/7zzo0UPzzUtuKbiLxGhqByzAnj2wdevn881rWKXkgoK7SD1NzcvXp2GVkgsK7iJJ\nZDLQb9gQBHnl5qU5KLiLpEmBXgqJgrtIIyS6mUj37tCmTcO2pUAv2aDgLtJEsTcT2bIF7ruv4cMq\noxToJVMU3EUyrKnDKqMU6KUpUgZ3M7vPzDab2asJ1n/fzFaFj1fNbK+Zdct8VUUKSyaGVUbFBvoe\nPTSOXlJLp+X+ADAh0Up3/5m7D3H3IcAPgD+5+0cZqp9IQcv0sEoIxtDHjqPX1bEST8rg7u7LgHSD\n9WRgYZNqJFKkshHoQdMgSHwZy7mbWXuCFv7iJGWmmVmVmVXV1NRkatciBUeBXrItkx2qpwD/nSwl\n4+7z3D3i7pGysrIM7lqkcDVHoFfHbOnJZHA/E6VkRJokW4E+SoG+dGQkuJtZZ2A08GQmticiyS+U\n6t49KNPUEThnnw1du2oUTjFKZyjkQuBF4Cgzqzaz881suplNjyk2EfhPd/8kWxUVKWX1L5TasiUz\nrft9+2DbtuC5RuEUF/NoYq6ZRSIRr6qqysm+RYrRggUwc2YQmM0+z7lnUp8+MGtW8GUjuWFmK9w9\nkqqcrlAVKRLZzteDcvaFRMHQQry2AAAGaklEQVRdpAg1d6Dv3l35+nyj4C5S5DI5g2UiH320f75e\nrfvcU3AXKSGJZrDM5CicWGrd546Cu0gJy9YonHjUum9eCu4icoDmyNlHqXWfHQruIpJUcwb6+q37\nc89VsG8sBXcRSVu2r5qtr7Y2fipHV9OmpuAuIo3SnPn6+upfTav8/YEU3EUko5q7dR8rNtB361ba\nrXsFdxHJmly27j/+OH7rvlQ6bRXcRaTZpWrdZ/oiq1ilMiRTwV1Ecipe677+RVaQvRZ+VGyg79w5\nSOsUcutewV1E8lau8vc7dgRpnfqt+86dg/0WQtBXcBeRgpDL/H3Ujh1BWqd+0O/ZEy66KAj4+RL4\nFdxFpKDlcnRO1MaNcPfdQcDPlw5cBXcRKRqpWvfNHfQhdx24Cu4iUvTyIaVT34YNMG1a9gK8gruI\nlKxcp3R27w5ujZgNCu4iIuQupfPuu5nZTn0pg7uZ3Wdmm83s1SRlxpjZKjN7zcz+lNkqiojkTjpB\nv0+fYLRMY1I8hx+elWqn1XJ/AJiQaKWZdQHuBE519wHANzNTNRGR/BUb9NevhzvvbHiKp317mDUr\nO/VLGdzdfRnwUZIiZwGPu/u7YfnNGaqbiEhBSre1P29eUDYbWmVgG/8LaG1mfwQ6Abe5+0MZ2K6I\nSFGprMxeMK8vE8G9FXAMcDxwEPCimf3V3f9Wv6CZTQOmARyerUSTiIhkZLRMNbDE3T9x9y3AMuDo\neAXdfZ67R9w9UlZWloFdi4hIPJkI7k8Cx5lZKzNrDxwLrM3AdkVEpJFSpmXMbCEwBuhhZtXAj4DW\nAO5+t7uvNbPfAauBfcC97p5w2KSIiGRfyuDu7pPTKPMz4GcZqZGIiDSZuXtudmxWA2xo5Nt7AFsy\nWJ1CUYrHXYrHDKV53KV4zNDw4+7j7ik7LXMW3JvCzKrcPZLrejS3UjzuUjxmKM3jLsVjhuwdt+aW\nEREpQgruIiJFqFCD+7xcVyBHSvG4S/GYoTSPuxSPGbJ03AWZcxcRkeQKteUuIiJJKLiLiBShggvu\nZjbBzN40s7fMbEau65MNZtbbzJaa2evhDVAuDZd3M7Pfm9m68N+uua5rNphZSzP7HzN7Onzd18yW\nh+f812bWJtd1zCQz62Jmj5nZG2a21sy+Wgrn2sy+F/7/ftXMFppZu2I81/FueJTo/Fpgbnj8q81s\nWGP3W1DB3cxaAncAJwIVwGQzq8htrbKiFrjC3SuAEcDF4XHOAP7g7l8G/hC+LkaXsv/8RDcCt7r7\nkcDHwPk5qVX23Ab8zt37EUy6t5YiP9dm1hO4BIi4+0CgJXAmxXmuH+DAGx4lOr8nAl8OH9OAuxq7\n04IK7sBw4C13f9vd/wksAk7LcZ0yzt03ufvK8PlOgj/2ngTH+mBY7EHgX3NTw+wxs17AycC94WsD\nxgGPhUWK6rjNrDPwNeBXAO7+T3ffRgmca4LpTw4ys1ZAe2ATRXiuE9zwKNH5PQ14yAN/BbqY2Rcb\ns99CC+49gfdiXleHy4qWmZUDQ4HlwCHuvilc9QFwSI6qlU1zgH8nmIQOoDuwzd1rw9fFds77AjXA\n/WEq6l4z60CRn2t3fx+4GXiXIKhvB1ZQ3Oc6VqLzm7EYV2jBvaSYWUdgMXCZu++IXefBGNaiGsdq\nZl8HNrv7ilzXpRm1AoYBd7n7UOAT6qVgivRcdyVopfYFDgM6kORezcUsW+e30IL7+0DvmNe9wmVF\nx8xaEwT2Be7+eLj4w+hPtPDfYrtf7UjgVDNbT5ByG0eQj+4S/nSH4jvn1UC1uy8PXz9GEOyL/VyP\nB95x9xp33wM8TnD+i/lcx0p0fjMW4wotuL8MfDnsUW9D0AHzVI7rlHFhnvlXwFp3vyVm1VPAOeHz\ncwhulFI03P0H7t7L3csJzu3z7l4JLAUmhcWK6rjd/QPgPTM7Klx0PPA6RX6uCdIxI8ysffj/PXrc\nRXuu60l0fp8Czg5HzYwAtsekbxrG3QvqAZwE/A34OzAz1/XJ0jEeR/AzbTWwKnycRJB//gOwDvgv\noFuu65rFz2AM8HT4/AjgJeAt4P8BbXNdvwwf6xCgKjzfvwG6lsK5Bn4MvAG8CjwMtC3Gcw0sJOhX\n2EPwS+38ROcXMIIRgX8H1hCMJmrUfjX9gIhIESq0tIyIiKRBwV1EpAgpuIuIFCEFdxGRIqTgLiJS\nhBTcRUSKkIK7iEgR+v9GN7JfUUEq9wAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdRPDy2zjxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9966fecc-f1f7-4829-93ca-6a324fd904ab"
      },
      "source": [
        "test_eval = full_model.evaluate(X_test, test_Y_one_hot, verbose=0)\n",
        "print('Test loss:', test_eval[0])\n",
        "print('Test accuracy:', test_eval[1])"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 1.5983738933563232\n",
            "Test accuracy: 0.4737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}