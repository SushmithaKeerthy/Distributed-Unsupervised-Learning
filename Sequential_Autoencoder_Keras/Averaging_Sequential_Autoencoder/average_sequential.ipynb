{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "average_sequential.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQkH9OF8reo7",
        "colab_type": "code",
        "outputId": "9d0e9b34-54de-4932-bbcd-29c976df84b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import keras\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import gzip\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import Input,Dense,Flatten,Dropout,merge,Reshape\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.models import Model,Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.optimizers import Adadelta, RMSprop,SGD,Adam\n",
        "from keras import regularizers\n",
        "from keras import backend as K\n",
        "from keras.utils import to_categorical\n",
        "from keras.datasets import mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train.astype('float32')/255\n",
        "X_test = X_test.astype('float32')/255\n",
        "X_train = X_train.reshape(len(X_train), np.prod(X_train.shape[1:]))\n",
        "X_test = X_test.reshape(len(X_test), np.prod(X_test.shape[1:]))\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "inChannel = 1\n",
        "x, y = 28, 28\n",
        "input_img = Input(shape = (x, y, inChannel))\n",
        "num_classes = 10\n",
        "\n",
        "X_train = X_train/np.max(X_train)\n",
        "X_test = X_test/np.max(X_test)\n",
        "\n",
        "print(np.max(X_train))\n",
        "train_X,valid_X,train_ground,valid_ground = train_test_split(X_train,\n",
        "                                                             X_train,\n",
        "                                                             test_size=0.2,\n",
        "                                                             random_state=13)\n",
        "\n",
        "input_img = Input(shape = (784,))\n",
        "num_classes = 10\n",
        "\n",
        "def Encoder_part(input_img):\n",
        "\n",
        "    encoded = Dense(units=256, activation='relu')(input_img)\n",
        "    encoded = Dense(units=128, activation='relu')(encoded)\n",
        "    encoded = Dense(units=64, activation='relu')(encoded)\n",
        "    encoded = Dense(units=32, activation='relu')(encoded)\n",
        "\n",
        "    return encoded\n",
        "\n",
        "def Decoder_part(encoded):\n",
        "        \n",
        "    decoded = Dense(units=64, activation='relu')(encoded)\n",
        "    decoded = Dense(units=128, activation='relu')(decoded)\n",
        "    decoded = Dense(units=256, activation='relu')(decoded)\n",
        "    decoded = Dense(units=784, activation='relu')(decoded)\n",
        "    \n",
        "    return decoded"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 784)\n",
            "(10000, 784)\n",
            "1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "URijeWSJ1uON",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "autoencoder1=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "autoencoder2=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder3=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder4=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "#autoencoder5=Model(input_img, Decoder_part(Encoder_part(input_img)))\n",
        "autoencoder_final=Model(input_img, Decoder_part(Encoder_part(input_img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAloY0wzso1i",
        "colab_type": "code",
        "outputId": "90250f1a-4f6f-41d2-95fc-a5a1ee7f28c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#encoder = Model(input_img, Encoder_part(input_img))\n",
        "\n",
        "autoencoder1.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "autoencoder2.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder3.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder4.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "#autoencoder5.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "autoencoder_final.compile(loss='mean_squared_error', optimizer = SGD())\n",
        "\n",
        "autoencoder1.summary()\n",
        "autoencoder2.summary()\n",
        "#autoencoder3.summary()\n",
        "#autoencoder4.summary()\n",
        "#autoencoder5.summary()\n",
        "autoencoder_final.summary()\n",
        "\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_40 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 784)               201488    \n",
            "=================================================================\n",
            "Total params: 489,136\n",
            "Trainable params: 489,136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 784)               201488    \n",
            "=================================================================\n",
            "Total params: 489,136\n",
            "Trainable params: 489,136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 128)               8320      \n",
            "_________________________________________________________________\n",
            "dense_59 (Dense)             (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dense_60 (Dense)             (None, 784)               201488    \n",
            "=================================================================\n",
            "Total params: 489,136\n",
            "Trainable params: 489,136\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwpv6AsLR7pN",
        "colab_type": "code",
        "outputId": "3e897610-4498-4f11-8947-d4e9cbb41c8a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "#print(train_X[1][1])\n",
        "#print(np.shape(train_X))\n",
        "batch_train_X = []\n",
        "batch_train_ground = []\n",
        "\n",
        "i=0\n",
        "while i < 48000:\n",
        "  x  = []\n",
        "  j = 0\n",
        "  while j<1000:\n",
        "    x.append(train_X[i])\n",
        "    j = j+1\n",
        "    i = i+1\n",
        "  print(i)\n",
        "  batch_train_X.append(x)\n",
        "print(\"done\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAyIO94uYz4R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#print(np.shape(train_X))\n",
        "print(np.shape(batch_train_ground))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xdl94PI_9ig8",
        "colab_type": "code",
        "outputId": "d75a2481-4fe8-46ae-ff61-017ac100bbac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 850
        }
      },
      "source": [
        "i=0\n",
        "while i < 48000:\n",
        "  x  = []\n",
        "  j = 0\n",
        "  while j<1000:\n",
        "    x.append(train_ground[i])\n",
        "    j = j+1\n",
        "    i = i+1\n",
        "  print(i)\n",
        "  batch_train_ground.append(x)\n",
        "print(\"done\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "11000\n",
            "12000\n",
            "13000\n",
            "14000\n",
            "15000\n",
            "16000\n",
            "17000\n",
            "18000\n",
            "19000\n",
            "20000\n",
            "21000\n",
            "22000\n",
            "23000\n",
            "24000\n",
            "25000\n",
            "26000\n",
            "27000\n",
            "28000\n",
            "29000\n",
            "30000\n",
            "31000\n",
            "32000\n",
            "33000\n",
            "34000\n",
            "35000\n",
            "36000\n",
            "37000\n",
            "38000\n",
            "39000\n",
            "40000\n",
            "41000\n",
            "42000\n",
            "43000\n",
            "44000\n",
            "45000\n",
            "46000\n",
            "47000\n",
            "48000\n",
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L066HK2Hd6hr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(batch_train_X[47][999])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3C8-BkEXo-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_X = valid_X[:1000]\n",
        "valid_ground = valid_ground[:1000]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuqOcgB7i8r1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(np.shape(train_X))\n",
        "print(np.shape(i))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COpqUI8478ih",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "3c91399d-f7ef-4b83-d250-9f2b2f24d5e4"
      },
      "source": [
        "count = 0\n",
        "number = 0\n",
        "for epo in range (0,100):\n",
        "  number = 0\n",
        "  print(\"entered first loop : \", epo)\n",
        "  for i,j in zip(batch_train_X,batch_train_ground):\n",
        "    print(\"entered second loop : \")\n",
        "    number = number + 1\n",
        "    i = [i]\n",
        "    j = [j]\n",
        "    print(\"**************************************************************************************************************8\")\n",
        "    print(\"batch : \" , number, \"epoch : \", epo)\n",
        "    if count == 0:\n",
        "      print(\"Entered if 1\", count)\n",
        "      autoencoder_train1 = autoencoder1.fit(i,j, batch_size=1000,epochs=1,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "      count = count+1\n",
        "    if count == 1:\n",
        "      print(\"Entered if 2\", count)\n",
        "      autoencoder_train2 = autoencoder2.fit(i,j, batch_size=1000,epochs=1,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "      count = count+1\n",
        "    #if count == 2:  \n",
        "    #  autoencoder_train3 = autoencoder3.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    #if count == 3:  \n",
        "    #  autoencoder_train4 = autoencoder4.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    #if count ==4:  \n",
        "    #  autoencoder_train5 = autoencoder5.fit(i,j, batch_size=1,epochs=epochs,verbose=1,validation_data=(valid_X, valid_ground))\n",
        "    if count ==2:\n",
        "      print(\"Entered if 3\", count)\n",
        "      count = 0\n",
        "      w1 = np.array(autoencoder1.get_weights())\n",
        "      w2 = np.array(autoencoder2.get_weights())\n",
        "      #w3 = autoencoder3.get_weights()\n",
        "      #w4 = autoencoder4.get_weights()\n",
        "      #w5 = autoencoder5.get_weights()\n",
        "      w_final = (w1+w2)/2\n",
        "      \n",
        "      print(\"Taken weights\")\n",
        "      \n",
        "      w_cur = np.array(autoencoder_final.get_weights())\n",
        "      w_final = (w_cur+w_final)/2\n",
        "\n",
        "      \n",
        "    \n",
        "      print(\"Meaned weights\")\n",
        "    \n",
        "      \n",
        "    \n",
        "      autoencoder_final.set_weights(w_final)\n",
        "      autoencoder1.set_weights(w_final)\n",
        "      autoencoder2.set_weights(w_final)\n",
        "      #autoencoder3.set_weights(w_final)\n",
        "      #autoencoder4.set_weights(w_final)\n",
        "      #autoencoder5.set_weights(w_final)\n",
        "      #autoencoder.set_weights(w_final)\n",
        "      \n",
        "      print(\"Going to the top again\")\n",
        "\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r1000/1000 [==============================] - 0s 19us/step - loss: 0.1051 - val_loss: 0.1086\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n",
            "entered second loop : \n",
            "**************************************************************************************************************8\n",
            "batch :  48 epoch :  99\n",
            "Entered if 1 0\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 17us/step - loss: 0.1060 - val_loss: 0.1086\n",
            "Entered if 2 1\n",
            "Train on 1000 samples, validate on 1000 samples\n",
            "Epoch 1/1\n",
            "1000/1000 [==============================] - 0s 18us/step - loss: 0.1060 - val_loss: 0.1086\n",
            "Entered if 3 2\n",
            "Taken weights\n",
            "Meaned weights\n",
            "Going to the top again\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SxxKWzsiw42",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = autoencoder_train1.history['loss']\n",
        "val_loss = autoencoder1.history['val_loss']\n",
        "epochs = range(75)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss - AE1')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "'''\n",
        "loss = autoencoder_train2.history['loss']\n",
        "val_loss = autoencoder_train2.history['val_loss']\n",
        "epochs = range(100)\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "weight'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2djvWZpsYHf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4290f1d0-3fb6-4446-bf80-83e3afeb9c14"
      },
      "source": [
        "wt_fn = autoencoder_final.get_weights()\n",
        "print(\"Weights taken\")\n",
        "print(np.shape(wt_fn[0]))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Weights taken\n",
            "(784, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u77euOJ2uSst",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "194ff821-9990-4771-e3dd-69ae3d3d1e28"
      },
      "source": [
        "train_Y_one_hot = to_categorical(y_train)\n",
        "test_Y_one_hot = to_categorical(y_test)\n",
        "print('Original label:', y_train[0])\n",
        "print('After conversion to one-hot:', train_Y_one_hot[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original label: 5\n",
            "After conversion to one-hot: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6VMaMoQAxAsR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_X,valid_X,y_train,y_valid = train_test_split(X_train,train_Y_one_hot,test_size=0.2,random_state=13)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SBrsWJpxDLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc(enco):\n",
        "    flat = enco\n",
        "    den = Dense(64, activation='relu')(flat)\n",
        "    out = Dense(num_classes, activation='softmax')(den)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-q3pY9mxIai",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model = Model(input_img,fc(Encoder_part(input_img)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NvQoDW9xMAf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l1,l2 in zip(full_model.layers[:6],autoencoder_final.layers[0:6]):\n",
        "    l1.set_weights(l2.get_weights())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOXgqx1qx3Mq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:6]:\n",
        "    layer.trainable = False\n",
        "full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf92vduBx_Lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "aea1dbd1-9b12-4eb1-9030-ebfb817e94ab"
      },
      "source": [
        "full_model.summary()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_4 (InputLayer)         (None, 784)               0         \n",
            "_________________________________________________________________\n",
            "dense_61 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dense_62 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dense_63 (Dense)             (None, 64)                8256      \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 32)                2080      \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 64)                2112      \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 10)                650       \n",
            "=================================================================\n",
            "Total params: 246,954\n",
            "Trainable params: 650\n",
            "Non-trainable params: 246,304\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77RiAnPdx_qa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:6]:\n",
        "    layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0P_Ki51gyF5Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f7515b9-03cb-4256-93d9-30cbd8b20cc3"
      },
      "source": [
        "classify_train = full_model.fit(train_X, y_train, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, y_valid))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 2.2920 - acc: 0.2038 - val_loss: 2.2826 - val_acc: 0.2555\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 4s 73us/step - loss: 2.2732 - acc: 0.2610 - val_loss: 2.2641 - val_acc: 0.2560\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.2552 - acc: 0.2878 - val_loss: 2.2462 - val_acc: 0.2839\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.2378 - acc: 0.3000 - val_loss: 2.2290 - val_acc: 0.3107\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.2210 - acc: 0.3190 - val_loss: 2.2122 - val_acc: 0.3266\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.2046 - acc: 0.3364 - val_loss: 2.1958 - val_acc: 0.3266\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.1886 - acc: 0.3351 - val_loss: 2.1802 - val_acc: 0.3562\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.1732 - acc: 0.3560 - val_loss: 2.1647 - val_acc: 0.3488\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.1581 - acc: 0.3614 - val_loss: 2.1498 - val_acc: 0.3454\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.1435 - acc: 0.3586 - val_loss: 2.1352 - val_acc: 0.3649\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.1294 - acc: 0.3712 - val_loss: 2.1212 - val_acc: 0.3671\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.1156 - acc: 0.3751 - val_loss: 2.1073 - val_acc: 0.3743\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.1022 - acc: 0.3785 - val_loss: 2.0941 - val_acc: 0.3767\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.0891 - acc: 0.3807 - val_loss: 2.0811 - val_acc: 0.3882\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.0764 - acc: 0.3881 - val_loss: 2.0684 - val_acc: 0.3877\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.0641 - acc: 0.3918 - val_loss: 2.0562 - val_acc: 0.3894\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.0521 - acc: 0.3953 - val_loss: 2.0443 - val_acc: 0.3912\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.0404 - acc: 0.3962 - val_loss: 2.0328 - val_acc: 0.3942\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.0290 - acc: 0.4016 - val_loss: 2.0213 - val_acc: 0.3947\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 2.0179 - acc: 0.4019 - val_loss: 2.0105 - val_acc: 0.4023\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 2.0072 - acc: 0.4050 - val_loss: 1.9998 - val_acc: 0.4027\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9968 - acc: 0.4081 - val_loss: 1.9894 - val_acc: 0.4056\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9867 - acc: 0.4097 - val_loss: 1.9793 - val_acc: 0.4088\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.9768 - acc: 0.4121 - val_loss: 1.9695 - val_acc: 0.4088\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9673 - acc: 0.4141 - val_loss: 1.9601 - val_acc: 0.4093\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9581 - acc: 0.4165 - val_loss: 1.9511 - val_acc: 0.4123\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9491 - acc: 0.4186 - val_loss: 1.9421 - val_acc: 0.4128\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9403 - acc: 0.4200 - val_loss: 1.9335 - val_acc: 0.4154\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9317 - acc: 0.4214 - val_loss: 1.9249 - val_acc: 0.4181\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9234 - acc: 0.4235 - val_loss: 1.9166 - val_acc: 0.4210\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.9154 - acc: 0.4246 - val_loss: 1.9086 - val_acc: 0.4234\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.9075 - acc: 0.4271 - val_loss: 1.9010 - val_acc: 0.4224\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8999 - acc: 0.4282 - val_loss: 1.8933 - val_acc: 0.4273\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8924 - acc: 0.4299 - val_loss: 1.8860 - val_acc: 0.4264\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8852 - acc: 0.4304 - val_loss: 1.8790 - val_acc: 0.4323\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8783 - acc: 0.4331 - val_loss: 1.8719 - val_acc: 0.4333\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8714 - acc: 0.4345 - val_loss: 1.8652 - val_acc: 0.4332\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8647 - acc: 0.4350 - val_loss: 1.8586 - val_acc: 0.4340\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8582 - acc: 0.4363 - val_loss: 1.8521 - val_acc: 0.4368\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8519 - acc: 0.4385 - val_loss: 1.8460 - val_acc: 0.4334\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8458 - acc: 0.4377 - val_loss: 1.8398 - val_acc: 0.4385\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8398 - acc: 0.4402 - val_loss: 1.8338 - val_acc: 0.4389\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8339 - acc: 0.4402 - val_loss: 1.8280 - val_acc: 0.4405\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8282 - acc: 0.4422 - val_loss: 1.8225 - val_acc: 0.4407\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8227 - acc: 0.4430 - val_loss: 1.8170 - val_acc: 0.4427\n",
            "Epoch 46/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8173 - acc: 0.4438 - val_loss: 1.8115 - val_acc: 0.4429\n",
            "Epoch 47/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8120 - acc: 0.4449 - val_loss: 1.8064 - val_acc: 0.4419\n",
            "Epoch 48/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.8069 - acc: 0.4447 - val_loss: 1.8013 - val_acc: 0.4438\n",
            "Epoch 49/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.8018 - acc: 0.4462 - val_loss: 1.7963 - val_acc: 0.4457\n",
            "Epoch 50/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7969 - acc: 0.4473 - val_loss: 1.7914 - val_acc: 0.4471\n",
            "Epoch 51/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7922 - acc: 0.4476 - val_loss: 1.7867 - val_acc: 0.4474\n",
            "Epoch 52/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7875 - acc: 0.4479 - val_loss: 1.7821 - val_acc: 0.4486\n",
            "Epoch 53/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7829 - acc: 0.4495 - val_loss: 1.7775 - val_acc: 0.4502\n",
            "Epoch 54/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7785 - acc: 0.4510 - val_loss: 1.7732 - val_acc: 0.4492\n",
            "Epoch 55/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7742 - acc: 0.4513 - val_loss: 1.7688 - val_acc: 0.4495\n",
            "Epoch 56/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7699 - acc: 0.4516 - val_loss: 1.7647 - val_acc: 0.4507\n",
            "Epoch 57/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7657 - acc: 0.4535 - val_loss: 1.7605 - val_acc: 0.4508\n",
            "Epoch 58/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7616 - acc: 0.4531 - val_loss: 1.7567 - val_acc: 0.4525\n",
            "Epoch 59/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7577 - acc: 0.4547 - val_loss: 1.7528 - val_acc: 0.4510\n",
            "Epoch 60/100\n",
            "48000/48000 [==============================] - 3s 72us/step - loss: 1.7538 - acc: 0.4539 - val_loss: 1.7487 - val_acc: 0.4540\n",
            "Epoch 61/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7501 - acc: 0.4551 - val_loss: 1.7449 - val_acc: 0.4547\n",
            "Epoch 62/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7463 - acc: 0.4562 - val_loss: 1.7412 - val_acc: 0.4561\n",
            "Epoch 63/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7427 - acc: 0.4565 - val_loss: 1.7376 - val_acc: 0.4560\n",
            "Epoch 64/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7391 - acc: 0.4572 - val_loss: 1.7341 - val_acc: 0.4569\n",
            "Epoch 65/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7356 - acc: 0.4582 - val_loss: 1.7308 - val_acc: 0.4562\n",
            "Epoch 66/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7323 - acc: 0.4577 - val_loss: 1.7273 - val_acc: 0.4572\n",
            "Epoch 67/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7289 - acc: 0.4593 - val_loss: 1.7241 - val_acc: 0.4582\n",
            "Epoch 68/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7257 - acc: 0.4594 - val_loss: 1.7208 - val_acc: 0.4582\n",
            "Epoch 69/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7224 - acc: 0.4601 - val_loss: 1.7177 - val_acc: 0.4598\n",
            "Epoch 70/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7193 - acc: 0.4612 - val_loss: 1.7147 - val_acc: 0.4590\n",
            "Epoch 71/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7163 - acc: 0.4608 - val_loss: 1.7116 - val_acc: 0.4595\n",
            "Epoch 72/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7132 - acc: 0.4615 - val_loss: 1.7085 - val_acc: 0.4608\n",
            "Epoch 73/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7103 - acc: 0.4620 - val_loss: 1.7056 - val_acc: 0.4618\n",
            "Epoch 74/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7075 - acc: 0.4620 - val_loss: 1.7028 - val_acc: 0.4628\n",
            "Epoch 75/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.7046 - acc: 0.4638 - val_loss: 1.6998 - val_acc: 0.4630\n",
            "Epoch 76/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.7018 - acc: 0.4629 - val_loss: 1.6971 - val_acc: 0.4636\n",
            "Epoch 77/100\n",
            "48000/48000 [==============================] - 3s 73us/step - loss: 1.6991 - acc: 0.4640 - val_loss: 1.6945 - val_acc: 0.4632\n",
            "Epoch 78/100\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.6965 - acc: 0.4644 - val_loss: 1.6919 - val_acc: 0.4639\n",
            "Epoch 79/100\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.6938 - acc: 0.4647 - val_loss: 1.6892 - val_acc: 0.4647\n",
            "Epoch 80/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6913 - acc: 0.4655 - val_loss: 1.6866 - val_acc: 0.4651\n",
            "Epoch 81/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6887 - acc: 0.4657 - val_loss: 1.6841 - val_acc: 0.4661\n",
            "Epoch 82/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6862 - acc: 0.4661 - val_loss: 1.6816 - val_acc: 0.4662\n",
            "Epoch 83/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6838 - acc: 0.4670 - val_loss: 1.6793 - val_acc: 0.4653\n",
            "Epoch 84/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6813 - acc: 0.4670 - val_loss: 1.6769 - val_acc: 0.4659\n",
            "Epoch 85/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6790 - acc: 0.4671 - val_loss: 1.6745 - val_acc: 0.4667\n",
            "Epoch 86/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6767 - acc: 0.4680 - val_loss: 1.6723 - val_acc: 0.4670\n",
            "Epoch 87/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6744 - acc: 0.4680 - val_loss: 1.6700 - val_acc: 0.4678\n",
            "Epoch 88/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6722 - acc: 0.4685 - val_loss: 1.6677 - val_acc: 0.4675\n",
            "Epoch 89/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6700 - acc: 0.4691 - val_loss: 1.6657 - val_acc: 0.4682\n",
            "Epoch 90/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6678 - acc: 0.4688 - val_loss: 1.6635 - val_acc: 0.4693\n",
            "Epoch 91/100\n",
            "48000/48000 [==============================] - 3s 72us/step - loss: 1.6657 - acc: 0.4698 - val_loss: 1.6613 - val_acc: 0.4697\n",
            "Epoch 92/100\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.6637 - acc: 0.4699 - val_loss: 1.6592 - val_acc: 0.4700\n",
            "Epoch 93/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6616 - acc: 0.4696 - val_loss: 1.6573 - val_acc: 0.4706\n",
            "Epoch 94/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6596 - acc: 0.4704 - val_loss: 1.6552 - val_acc: 0.4715\n",
            "Epoch 95/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6575 - acc: 0.4704 - val_loss: 1.6532 - val_acc: 0.4708\n",
            "Epoch 96/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6556 - acc: 0.4707 - val_loss: 1.6513 - val_acc: 0.4712\n",
            "Epoch 97/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6537 - acc: 0.4711 - val_loss: 1.6494 - val_acc: 0.4721\n",
            "Epoch 98/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6518 - acc: 0.4722 - val_loss: 1.6475 - val_acc: 0.4718\n",
            "Epoch 99/100\n",
            "48000/48000 [==============================] - 3s 70us/step - loss: 1.6499 - acc: 0.4719 - val_loss: 1.6458 - val_acc: 0.4720\n",
            "Epoch 100/100\n",
            "48000/48000 [==============================] - 3s 71us/step - loss: 1.6480 - acc: 0.4728 - val_loss: 1.6439 - val_acc: 0.4726\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOqbITPayIUb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model.save_weights('classification_complete.h5')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eANYBVss4D7f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in full_model.layers[0:19]:\n",
        "    layer.trainable = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sceRvyf4OFa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "full_model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(),metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMgTu1Zg4RfC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "db0bccb7-e3f5-4c33-c86a-8ce9ac219fdd"
      },
      "source": [
        "classify_train = full_model.fit(train_X, y_train, batch_size=64,epochs=100,verbose=1,validation_data=(valid_X, y_valid))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/100\n",
            "48000/48000 [==============================] - 6s 125us/step - loss: 0.2635 - acc: 0.9216 - val_loss: 0.2360 - val_acc: 0.9307\n",
            "Epoch 2/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.1277 - acc: 0.9625 - val_loss: 0.1559 - val_acc: 0.9572\n",
            "Epoch 3/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0961 - acc: 0.9717 - val_loss: 0.1266 - val_acc: 0.9650\n",
            "Epoch 4/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0827 - acc: 0.9755 - val_loss: 0.1242 - val_acc: 0.9649\n",
            "Epoch 5/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0638 - acc: 0.9810 - val_loss: 0.1089 - val_acc: 0.9718\n",
            "Epoch 6/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0573 - acc: 0.9829 - val_loss: 0.1412 - val_acc: 0.9641\n",
            "Epoch 7/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0496 - acc: 0.9851 - val_loss: 0.1331 - val_acc: 0.9691\n",
            "Epoch 8/100\n",
            "48000/48000 [==============================] - 5s 115us/step - loss: 0.0470 - acc: 0.9855 - val_loss: 0.1093 - val_acc: 0.9748\n",
            "Epoch 9/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0418 - acc: 0.9882 - val_loss: 0.1026 - val_acc: 0.9736\n",
            "Epoch 10/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0378 - acc: 0.9889 - val_loss: 0.1253 - val_acc: 0.9727\n",
            "Epoch 11/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0339 - acc: 0.9902 - val_loss: 0.1138 - val_acc: 0.9737\n",
            "Epoch 12/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.1097 - val_acc: 0.9766\n",
            "Epoch 13/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0287 - acc: 0.9917 - val_loss: 0.1328 - val_acc: 0.9717\n",
            "Epoch 14/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0293 - acc: 0.9914 - val_loss: 0.1287 - val_acc: 0.9736\n",
            "Epoch 15/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0257 - acc: 0.9928 - val_loss: 0.1302 - val_acc: 0.9740\n",
            "Epoch 16/100\n",
            "48000/48000 [==============================] - 5s 115us/step - loss: 0.0175 - acc: 0.9951 - val_loss: 0.1354 - val_acc: 0.9735\n",
            "Epoch 17/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0230 - acc: 0.9934 - val_loss: 0.1570 - val_acc: 0.9689\n",
            "Epoch 18/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0252 - acc: 0.9933 - val_loss: 0.1089 - val_acc: 0.9778\n",
            "Epoch 19/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0181 - acc: 0.9948 - val_loss: 0.1323 - val_acc: 0.9755\n",
            "Epoch 20/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0165 - acc: 0.9953 - val_loss: 0.1142 - val_acc: 0.9782\n",
            "Epoch 21/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0263 - acc: 0.9929 - val_loss: 0.1123 - val_acc: 0.9792\n",
            "Epoch 22/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0157 - acc: 0.9955 - val_loss: 0.1308 - val_acc: 0.9768\n",
            "Epoch 23/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0125 - acc: 0.9965 - val_loss: 0.1172 - val_acc: 0.9789\n",
            "Epoch 24/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0187 - acc: 0.9952 - val_loss: 0.1379 - val_acc: 0.9782\n",
            "Epoch 25/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0173 - acc: 0.9954 - val_loss: 0.1445 - val_acc: 0.9755\n",
            "Epoch 26/100\n",
            "48000/48000 [==============================] - 6s 116us/step - loss: 0.0146 - acc: 0.9961 - val_loss: 0.1093 - val_acc: 0.9792\n",
            "Epoch 27/100\n",
            "48000/48000 [==============================] - 6s 115us/step - loss: 0.0166 - acc: 0.9959 - val_loss: 0.1266 - val_acc: 0.9778\n",
            "Epoch 28/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0165 - acc: 0.9959 - val_loss: 0.1263 - val_acc: 0.9794\n",
            "Epoch 29/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0141 - acc: 0.9962 - val_loss: 0.1572 - val_acc: 0.9707\n",
            "Epoch 30/100\n",
            "48000/48000 [==============================] - 5s 114us/step - loss: 0.0135 - acc: 0.9965 - val_loss: 0.1312 - val_acc: 0.9778\n",
            "Epoch 31/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0136 - acc: 0.9968 - val_loss: 0.1257 - val_acc: 0.9785\n",
            "Epoch 32/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0117 - acc: 0.9970 - val_loss: 0.1269 - val_acc: 0.9785\n",
            "Epoch 33/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0103 - acc: 0.9975 - val_loss: 0.1413 - val_acc: 0.9761\n",
            "Epoch 34/100\n",
            "48000/48000 [==============================] - 6s 116us/step - loss: 0.0172 - acc: 0.9958 - val_loss: 0.1205 - val_acc: 0.9772\n",
            "Epoch 35/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0139 - acc: 0.9963 - val_loss: 0.1313 - val_acc: 0.9779\n",
            "Epoch 36/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0152 - acc: 0.9966 - val_loss: 0.1377 - val_acc: 0.9774\n",
            "Epoch 37/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0110 - acc: 0.9974 - val_loss: 0.1234 - val_acc: 0.9791\n",
            "Epoch 38/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0081 - acc: 0.9979 - val_loss: 0.1398 - val_acc: 0.9782\n",
            "Epoch 39/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0094 - acc: 0.9973 - val_loss: 0.1434 - val_acc: 0.9795\n",
            "Epoch 40/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0117 - acc: 0.9977 - val_loss: 0.1518 - val_acc: 0.9732\n",
            "Epoch 41/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0196 - acc: 0.9960 - val_loss: 0.1908 - val_acc: 0.9721\n",
            "Epoch 42/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0122 - acc: 0.9969 - val_loss: 0.1772 - val_acc: 0.9761\n",
            "Epoch 43/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0127 - acc: 0.9976 - val_loss: 0.1624 - val_acc: 0.9763\n",
            "Epoch 44/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0122 - acc: 0.9974 - val_loss: 0.1717 - val_acc: 0.9756\n",
            "Epoch 45/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0121 - acc: 0.9977 - val_loss: 0.1305 - val_acc: 0.9803\n",
            "Epoch 46/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0036 - acc: 0.9992 - val_loss: 0.1685 - val_acc: 0.9774\n",
            "Epoch 47/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0110 - acc: 0.9975 - val_loss: 0.1472 - val_acc: 0.9798\n",
            "Epoch 48/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0124 - acc: 0.9975 - val_loss: 0.1485 - val_acc: 0.9752\n",
            "Epoch 49/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0154 - acc: 0.9967 - val_loss: 0.1679 - val_acc: 0.9753\n",
            "Epoch 50/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0125 - acc: 0.9974 - val_loss: 0.1435 - val_acc: 0.9772\n",
            "Epoch 51/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0091 - acc: 0.9980 - val_loss: 0.1458 - val_acc: 0.9785\n",
            "Epoch 52/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0081 - acc: 0.9983 - val_loss: 0.1457 - val_acc: 0.9789\n",
            "Epoch 53/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0090 - acc: 0.9979 - val_loss: 0.1552 - val_acc: 0.9785\n",
            "Epoch 54/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0131 - acc: 0.9976 - val_loss: 0.1591 - val_acc: 0.9779\n",
            "Epoch 55/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0120 - acc: 0.9977 - val_loss: 0.1757 - val_acc: 0.9782\n",
            "Epoch 56/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0185 - acc: 0.9965 - val_loss: 0.1672 - val_acc: 0.9748\n",
            "Epoch 57/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0090 - acc: 0.9981 - val_loss: 0.1344 - val_acc: 0.9808\n",
            "Epoch 58/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0023 - acc: 0.9994 - val_loss: 0.1534 - val_acc: 0.9812\n",
            "Epoch 59/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0194 - acc: 0.9966 - val_loss: 0.1748 - val_acc: 0.9748\n",
            "Epoch 60/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0140 - acc: 0.9972 - val_loss: 0.1555 - val_acc: 0.9761\n",
            "Epoch 61/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0183 - acc: 0.9967 - val_loss: 0.1896 - val_acc: 0.9774\n",
            "Epoch 62/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0142 - acc: 0.9972 - val_loss: 0.1835 - val_acc: 0.9765\n",
            "Epoch 63/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0088 - acc: 0.9980 - val_loss: 0.1775 - val_acc: 0.9756\n",
            "Epoch 64/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0042 - acc: 0.9988 - val_loss: 0.1541 - val_acc: 0.9807\n",
            "Epoch 65/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0021 - acc: 0.9994 - val_loss: 0.1605 - val_acc: 0.9793\n",
            "Epoch 66/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0129 - acc: 0.9973 - val_loss: 0.1656 - val_acc: 0.9778\n",
            "Epoch 67/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0082 - acc: 0.9980 - val_loss: 0.1754 - val_acc: 0.9784\n",
            "Epoch 68/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0180 - acc: 0.9969 - val_loss: 0.1477 - val_acc: 0.9778\n",
            "Epoch 69/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0043 - acc: 0.9988 - val_loss: 0.1439 - val_acc: 0.9807\n",
            "Epoch 70/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0060 - acc: 0.9986 - val_loss: 0.1614 - val_acc: 0.9780\n",
            "Epoch 71/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0056 - acc: 0.9990 - val_loss: 0.1712 - val_acc: 0.9792\n",
            "Epoch 72/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0080 - acc: 0.9983 - val_loss: 0.1790 - val_acc: 0.9780\n",
            "Epoch 73/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0077 - acc: 0.9980 - val_loss: 0.1523 - val_acc: 0.9800\n",
            "Epoch 74/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0057 - acc: 0.9990 - val_loss: 0.1517 - val_acc: 0.9806\n",
            "Epoch 75/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0133 - acc: 0.9976 - val_loss: 0.1711 - val_acc: 0.9781\n",
            "Epoch 76/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0053 - acc: 0.9988 - val_loss: 0.1914 - val_acc: 0.9766\n",
            "Epoch 77/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0054 - acc: 0.9988 - val_loss: 0.1467 - val_acc: 0.9808\n",
            "Epoch 78/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0036 - acc: 0.9994 - val_loss: 0.1745 - val_acc: 0.9798\n",
            "Epoch 79/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0146 - acc: 0.9974 - val_loss: 0.1666 - val_acc: 0.9785\n",
            "Epoch 80/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0081 - acc: 0.9985 - val_loss: 0.1602 - val_acc: 0.9776\n",
            "Epoch 81/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0118 - acc: 0.9976 - val_loss: 0.1639 - val_acc: 0.9782\n",
            "Epoch 82/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0092 - acc: 0.9983 - val_loss: 0.1575 - val_acc: 0.9782\n",
            "Epoch 83/100\n",
            "48000/48000 [==============================] - 6s 116us/step - loss: 0.0062 - acc: 0.9987 - val_loss: 0.1719 - val_acc: 0.9771\n",
            "Epoch 84/100\n",
            "48000/48000 [==============================] - 6s 115us/step - loss: 0.0051 - acc: 0.9991 - val_loss: 0.1898 - val_acc: 0.9782\n",
            "Epoch 85/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0106 - acc: 0.9984 - val_loss: 0.1825 - val_acc: 0.9754\n",
            "Epoch 86/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0072 - acc: 0.9983 - val_loss: 0.1907 - val_acc: 0.9772\n",
            "Epoch 87/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0028 - acc: 0.9994 - val_loss: 0.1577 - val_acc: 0.9812\n",
            "Epoch 88/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 6.3342e-04 - acc: 0.9999 - val_loss: 0.1827 - val_acc: 0.9788\n",
            "Epoch 89/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0190 - acc: 0.9967 - val_loss: 0.1696 - val_acc: 0.9786\n",
            "Epoch 90/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0056 - acc: 0.9986 - val_loss: 0.1630 - val_acc: 0.9812\n",
            "Epoch 91/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0085 - acc: 0.9985 - val_loss: 0.1806 - val_acc: 0.9769\n",
            "Epoch 92/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0049 - acc: 0.9991 - val_loss: 0.1884 - val_acc: 0.9798\n",
            "Epoch 93/100\n",
            "48000/48000 [==============================] - 5s 111us/step - loss: 0.0100 - acc: 0.9985 - val_loss: 0.2043 - val_acc: 0.9768\n",
            "Epoch 94/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0119 - acc: 0.9979 - val_loss: 0.2199 - val_acc: 0.9761\n",
            "Epoch 95/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0123 - acc: 0.9977 - val_loss: 0.1372 - val_acc: 0.9812\n",
            "Epoch 96/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0042 - acc: 0.9993 - val_loss: 0.1551 - val_acc: 0.9814\n",
            "Epoch 97/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0031 - acc: 0.9996 - val_loss: 0.1917 - val_acc: 0.9801\n",
            "Epoch 98/100\n",
            "48000/48000 [==============================] - 5s 113us/step - loss: 0.0228 - acc: 0.9966 - val_loss: 0.1647 - val_acc: 0.9793\n",
            "Epoch 99/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0082 - acc: 0.9984 - val_loss: 0.1752 - val_acc: 0.9807\n",
            "Epoch 100/100\n",
            "48000/48000 [==============================] - 5s 112us/step - loss: 0.0069 - acc: 0.9989 - val_loss: 0.1839 - val_acc: 0.9791\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Ovj_adgzeyL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "outputId": "8eda1adc-8c0b-49d8-9029-bb1ea89fcfd1"
      },
      "source": [
        "full_model.save_weights('classification_complete.h5')\n",
        "accuracy = classify_train.history['acc']\n",
        "val_accuracy = classify_train.history['val_acc']\n",
        "loss = classify_train.history['loss']\n",
        "val_loss = classify_train.history['val_loss']\n",
        "epochs = range(len(accuracy))\n",
        "plt.plot(epochs, accuracy, 'bo', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAEICAYAAACzliQjAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqOgriqIkECr6iqIoCYSKvqIoSgLx/yodn3MLUJJ/AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNdRPDy2zjxv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "064a1670-f98e-4d6c-b1cf-4838b41f3216"
      },
      "source": [
        "test_eval = full_model.evaluate(X_test, test_Y_one_hot, verbose=0)\n",
        "print('Test loss:', test_eval[0])\n",
        "print('Test accuracy:', test_eval[1])"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test loss: 0.17859678938569792\n",
            "Test accuracy: 0.9791\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}